{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c41ba4a-c353-4b36-b869-185627598a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "src = r\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML\\src\" # copy the location of the folder that has paths.py file using (ctrl + shift + C ) and paste it here    \n",
    "sys.path.append(src)\n",
    "\n",
    "from paths import *\n",
    "\n",
    "get_paths()\n",
    "\n",
    "sys.path.append(SRC_PATH)\n",
    "\n",
    "from pipeline import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3b2184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_42\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       6.24 GB / 31.92 GB (19.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_42\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189591.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 331 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 15): ['BindRequest', 'AHG', 'CEQ', 'FT', 'PAP', 'Carolina_Casualty_Insurance_Co', 'Commerce_West_Insurance_Company', 'Everest_National_Insurance_Company', 'Great_American_Insurance_Group', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'NorGuard_Insurance_Company', 'Selective_Ins_Co_of_America_Selective_', 'Starr_Indemnity_STARR_', 'United_Fire_Casualty_Group', 'West_American_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 189): ['BC', 'CT', 'DE', 'HI', 'LA', 'MA', 'ME', 'PR', 'VT', 'WV', 'CYB', 'DF', 'EPLDO', 'GARAGE_PKG', 'PAF', 'PERQK', 'PWO', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Chartis', 'Chartis_Casualty_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford', 'Hartford_Fire_Insurance_Company', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Hartford_', 'Sentinel_Insurance_Company_Limited', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 189 | ['BC', 'CT', 'DE', 'HI', 'LA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 146 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 142 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t3.0s = Fit runtime\n",
      "\t146 features in original data used to generate 146 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 26.63s of the 26.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4091.3 GB\n",
      "\t0.9348\t = Validation score   (accuracy)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 24.50s of the 24.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4091.3 GB\n",
      "\t0.9372\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 23.91s of the 23.91s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/4091.3 GB\n",
      "\t0.9092\t = Validation score   (accuracy)\n",
      "\t12.99s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 10.47s of the 10.47s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/4091.3 GB\n",
      "\t0.9096\t = Validation score   (accuracy)\n",
      "\t14.26s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 26.63s of the -5.95s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.688, 'LightGBMXT': 0.25, 'RandomForestGini': 0.062}\n",
      "\t0.9384\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 36.14s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 29672.6 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_42\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.34 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_123\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189527.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 331 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 15): ['BindRequest', 'AHG', 'CEQ', 'FT', 'PAP', 'Carolina_Casualty_Insurance_Co', 'Commerce_West_Insurance_Company', 'Everest_National_Insurance_Company', 'Great_American_Insurance_Group', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'NorGuard_Insurance_Company', 'Selective_Ins_Co_of_America_Selective_', 'Starr_Indemnity_STARR_', 'United_Fire_Casualty_Group', 'West_American_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 189): ['BC', 'CT', 'DE', 'HI', 'LA', 'MA', 'ME', 'PR', 'VT', 'WV', 'CYB', 'DF', 'EPLDO', 'GARAGE_PKG', 'PAF', 'PERQK', 'PWO', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Chartis', 'Chartis_Casualty_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford', 'Hartford_Fire_Insurance_Company', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Hartford_', 'Sentinel_Insurance_Company_Limited', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 189 | ['BC', 'CT', 'DE', 'HI', 'LA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 146 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 142 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t4.4s = Fit runtime\n",
      "\t146 features in original data used to generate 146 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 25.17s of the 25.16s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4091.2 GB\n",
      "\t0.9348\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 24.54s of the 24.54s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4091.2 GB\n",
      "\t0.9372\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 23.84s of the 23.84s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/4091.2 GB\n",
      "\t0.9092\t = Validation score   (accuracy)\n",
      "\t12.75s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 9.58s of the 9.57s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/4091.3 GB\n",
      "\t0.9096\t = Validation score   (accuracy)\n",
      "\t15.33s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 25.17s of the -7.47s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.688, 'LightGBMXT': 0.25, 'RandomForestGini': 0.062}\n",
      "\t0.9384\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 37.65s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35108.9 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_modeldata_123\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_42\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.37 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_42\"\n",
      "Train Data Rows:    712\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189558.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 569, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 29.93s of the 29.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.4 GB\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 29.58s of the 29.58s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.4 GB\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 29.09s of the 29.09s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 28.42s of the 28.41s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 27.71s of the 27.71s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8112\t = Validation score   (accuracy)\n",
      "\t5.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 22.62s of the 22.62s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 21.93s of the 21.93s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 21.20s of the 21.20s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.3 GB\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 19.35s of the 19.35s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 18.87s of the 18.87s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 14.62s of the 14.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.93s of the 12.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 18.12s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19011.2 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_42\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.18 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_123\"\n",
      "Train Data Rows:    712\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189373.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 569, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 29.93s of the 29.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 29.50s of the 29.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 29.00s of the 29.00s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 28.33s of the 28.32s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 27.65s of the 27.64s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8112\t = Validation score   (accuracy)\n",
      "\t4.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 23.11s of the 23.10s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 22.35s of the 22.35s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 21.63s of the 21.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 20.69s of the 20.69s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 20.36s of the 20.36s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 17.80s of the 17.80s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.93s of the 16.23s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 13.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 19004.6 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_titanic_123\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_42\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.18 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_42\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189370.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.96s of the 29.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "No improvement since epoch 6: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 29.25s of the 29.25s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 28.90s of the 28.89s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 28.36s of the 28.35s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 27.68s of the 27.67s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 26.99s of the 26.99s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 26.69s of the 26.69s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 26.04s of the 26.04s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 25.35s of the 25.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 25.04s of the 25.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 23.67s of the 23.67s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.96s of the 22.75s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7.37s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3681.6 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_42\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.18 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_123\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189369.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.97s of the 29.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "No improvement since epoch 6: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 29.09s of the 29.09s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 28.73s of the 28.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 28.15s of the 28.15s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 27.43s of the 27.43s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 26.71s of the 26.71s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 26.41s of the 26.40s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 25.70s of the 25.70s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 24.99s of the 24.99s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 24.68s of the 24.68s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 23.45s of the 23.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.97s of the 23.09s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7.03s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3686.5 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_train_123\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_42\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.18 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_42\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189368.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.94s of the 29.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 28.45s of the 28.44s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 28.17s of the 28.17s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 27.90s of the 27.90s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 27.19s of the 27.19s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 26.54s of the 26.54s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 26.08s of the 26.08s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 25.40s of the 25.39s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 24.68s of the 24.67s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 24.54s of the 24.54s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 24.21s of the 24.21s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.94s of the 23.81s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.3s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 593.9 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_42\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_123\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.18 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 4096 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_123\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4189368.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.96s of the 29.95s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 29.39s of the 29.39s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 29.14s of the 29.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 28.88s of the 28.88s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 28.16s of the 28.16s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 27.50s of the 27.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 27.03s of the 27.03s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 26.33s of the 26.33s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 25.67s of the 25.67s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 25.49s of the 25.49s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 25.14s of the 25.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4091.2 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.96s of the 24.41s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.7s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 626.8 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\Desktop\\Coding mo\\AutoML proj1\\task 1\\AutoML Project Molham\\outputs\\metrics\\automl_metrics\\autogluon_wine_123\")\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Task 1+9 ----------------\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import torch\n",
    "\n",
    "\n",
    "def loop_dfs_and_evaluate_autogluon():\n",
    "    for dataset_name in data_dict_classification_only.keys():\n",
    "        dataset_summary = []\n",
    "        runtime_memory_rows = []\n",
    "        dataset_base = dataset_name.replace(\".csv\", \"\")\n",
    "\n",
    "        for seed in RANDOM_SEEDS:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            \n",
    "            try:\n",
    "                torch.manual_seed(seed)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            df_train = splits_dict[dataset_name]['train']\n",
    "            df_test = splits_dict[dataset_name]['test']\n",
    "            task_type = tasks_dict_classification_only[dataset_name]\n",
    "\n",
    "            start_time = time.time()\n",
    "            predictor = TabularPredictor(\n",
    "                label=TARGET_COLS[dataset_name],\n",
    "                problem_type=task_type,\n",
    "                path=os.path.join(AUTOML_METRICS_PATH, f\"autogluon_{dataset_base}_{seed}\")\n",
    "            ).fit(\n",
    "                df_train,\n",
    "                time_limit=TIME_BUDGET,\n",
    "                memory_limit=MEMORY_LIMIT\n",
    "            )\n",
    "            training_runtime_sec = round(time.time() - start_time, 3)\n",
    "            \n",
    "            infer_start = time.time()\n",
    "            results = evaluate_and_save_results_general(df_train, df_test, predictor, dataset_name, framework='autogluon')\n",
    "            inference_time_per_sample = round((time.time() - infer_start) / len(df_test), 6)\n",
    "            cpu_usage_percent = psutil.cpu_percent(interval=None)\n",
    "\n",
    "            model_dir = predictor.path\n",
    "            total_bytes = 0\n",
    "            for root, dirs, files in os.walk(model_dir):\n",
    "                for f in files:\n",
    "                    fp = os.path.join(root, f)\n",
    "                    total_bytes += os.path.getsize(fp)\n",
    "            model_size_mb = round(total_bytes / (1024 * 1024), 3)\n",
    "\n",
    "            memory_usage_mb = get_memory_usage_mb()\n",
    "\n",
    "            runtime_memory_rows.append({\n",
    "                \"dataset\": dataset_name,\n",
    "                \"seed\": seed,\n",
    "                \"training_runtime_sec\": training_runtime_sec,\n",
    "                \"model_size_MB\": model_size_mb,\n",
    "                \"memory_usage_MB\": memory_usage_mb,\n",
    "                \"inference_time_per_sample_sec\": inference_time_per_sample,\n",
    "                \"cpu_usage_percent\": cpu_usage_percent\n",
    "            })\n",
    "\n",
    "            dataset_summary.append({\n",
    "                'dataset': dataset_name, \n",
    "                'seed': seed, \n",
    "                **results, \n",
    "                'runtime': training_runtime_sec\n",
    "            })\n",
    "\n",
    "            del predictor\n",
    "            gc.collect()\n",
    "\n",
    "        output_path = os.path.join(AUTOML_METRICS_PATH, f\"autogluon_{dataset_base}_summary.csv\")\n",
    "        pd.DataFrame(dataset_summary).to_csv(output_path, index=False)\n",
    "\n",
    "        task1_output_path = os.path.join(RUNTIME_MEMORY_TASK1_PATH, f\"autogluon_{dataset_base}_task1_runtime_memory_metrics.csv\")\n",
    "        pd.DataFrame(runtime_memory_rows).to_csv(task1_output_path, index=False)\n",
    "\n",
    "        json_path = os.path.join(RUNTIME_MEMORY_TASK1_PATH, f\"autogluon_{dataset_base}_runtime_memory_metrics_in_json.json\")\n",
    "\n",
    "        \n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(runtime_memory_rows, f, indent=2)\n",
    "\n",
    "loop_dfs_and_evaluate_autogluon()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85aebc8-5041-4f4d-bc98-e544844a0253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>0.927465</td>\n",
       "      <td>0.927521</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>37.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>0.927465</td>\n",
       "      <td>0.927521</td>\n",
       "      <td>0.927417</td>\n",
       "      <td>39.331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  accuracy        f1  precision    recall  runtime\n",
       "0  modeldata.csv    42  0.927417  0.927465   0.927521  0.927417   37.111\n",
       "1  modeldata.csv   123  0.927417  0.927465   0.927521  0.927417   39.331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_task1_runtime_memory_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>37.111</td>\n",
       "      <td>1089.093</td>\n",
       "      <td>4772.992</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>39.331</td>\n",
       "      <td>1775.176</td>\n",
       "      <td>4741.988</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>80.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  modeldata.csv    42                37.111       1089.093         4772.992                       0.000034               49.1\n",
       "1  modeldata.csv   123                39.331       1775.176         4741.988                       0.000071               80.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792086</td>\n",
       "      <td>0.792152</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>18.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.792086</td>\n",
       "      <td>0.792152</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>14.659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  accuracy        f1  precision    recall  runtime\n",
       "0  titanic.csv    42  0.793296  0.792086   0.792152  0.793296   18.873\n",
       "1  titanic.csv   123  0.793296  0.792086   0.792152  0.793296   14.659"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_task1_runtime_memory_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>18.873</td>\n",
       "      <td>27.206</td>\n",
       "      <td>4930.605</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>14.659</td>\n",
       "      <td>27.206</td>\n",
       "      <td>4933.918</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>95.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  titanic.csv    42                18.873         27.206         4930.605                       0.000091               92.0\n",
       "1  titanic.csv   123                14.659         27.206         4933.918                       0.000095               95.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>8.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966411</td>\n",
       "      <td>0.969444</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>7.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  accuracy        f1  precision    recall  runtime\n",
       "0  train.csv    42  0.966667  0.966411   0.969444  0.966667    8.201\n",
       "1  train.csv   123  0.966667  0.966411   0.969444  0.966667    7.873"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_task1_runtime_memory_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>8.201</td>\n",
       "      <td>4.409</td>\n",
       "      <td>4934.707</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>97.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>7.873</td>\n",
       "      <td>4.409</td>\n",
       "      <td>4935.156</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>97.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  train.csv    42                 8.201          4.409         4934.707                       0.000734               97.7\n",
       "1  train.csv   123                 7.873          4.409         4935.156                       0.000667               97.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  accuracy   f1  precision  recall  runtime\n",
       "0  wine.csv    42       1.0  1.0        1.0     1.0    7.007\n",
       "1  wine.csv   123       1.0  1.0        1.0     1.0    6.441"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_task1_runtime_memory_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>7.007</td>\n",
       "      <td>5.513</td>\n",
       "      <td>4935.832</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>6.441</td>\n",
       "      <td>5.513</td>\n",
       "      <td>4935.992</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>72.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  wine.csv    42                 7.007          5.513         4935.832                       0.001544               67.6\n",
       "1  wine.csv   123                 6.441          5.513         4935.992                       0.001835               72.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in data_dict_classification_only.keys():\n",
    "    dataset_base = dataset_name.replace(\".csv\", \"\")\n",
    "    csv_file1  = os.path.join(AUTOML_METRICS_PATH, f\"autogluon_{dataset_base}_summary.csv\")\n",
    "    csv_file2  = os.path.join(RUNTIME_MEMORY_TASK1_PATH, f\"autogluon_{dataset_base}_task1_runtime_memory_metrics.csv\")\n",
    "    json_file1 = os.path.join(RUNTIME_MEMORY_TASK1_PATH, f\"autogluon_{dataset_base}_runtime_memory_metrics_in_json.json\")\n",
    "\n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_summary.csv\")\n",
    "    df1 = pd.read_csv(csv_file1)\n",
    "    display(df1)\n",
    "\n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_task1_runtime_memory_metrics.csv\")\n",
    "    df2 = pd.read_csv(csv_file2)\n",
    "    display(df2)\n",
    "\n",
    "    # print(f\"JSON for {dataset_base}: autogluon_{dataset_base}_runtime_memory_metrics_in_json.json\")\n",
    "    # df3 = pd.read_json(json_file1)\n",
    "    # display(df3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286cac66-39fc-4d48-b0c8-38ea9204c202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpdc9bsrt5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4091.14 GB / 4096.00 GB (99.9%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpdc9bsrt5\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043551.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 325 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['BindRequest', 'PAP', 'XEO', 'Associated_Industries_Insurance_Co_Inc', 'C_N_A_Surety', 'Commerce_West_Insurance_Company', 'Crum_Forster_Insurance', 'Explorer_Insurance_Company', 'Granite_State_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Metcom_Excess', 'Mt_Hawley_Insurance_Company', 'Norman_Spencer', 'RSUI_Indemnity_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Twin_City_Fire_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'Western_World']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 192): ['BC', 'CT', 'DE', 'FL', 'HI', 'MA', 'ME', 'MS', 'NC', 'ND', 'PR', 'SC', 'VT', 'WV', 'AHG', 'BR', 'CEQ', 'CPKG_AUTO', 'DF', 'EPLDO', 'FT', 'GARAGE_PKG', 'PAF', 'PERQK', 'POLL', 'PWO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Hartford', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'McGowan', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_Nat_Mut_Cas_Ins_Co_Penn_National_', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Contractors_Ins_Co_RRG_LLC', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Casualty_Inc_Co_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'US_Underwriters_Insurance_Co', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 192 | ['BC', 'CT', 'DE', 'FL', 'HI', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 137 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 133 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t4.8s = Fit runtime\n",
      "\t137 features in original data used to generate 137 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 20.40 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.83s of the 4.82s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.9268\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 4.10s of the 4.10s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.9252\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3.41s of the 3.41s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.906\t = Validation score   (accuracy)\n",
      "\t14.0s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.83s of the -11.10s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9268\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21.31s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 382287.4 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpdc9bsrt5\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp95mnga3_\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.97 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp95mnga3_\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043569.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 329 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 17): ['BindRequest', 'BC', 'AmTrust', 'American_Guarantee', 'Chartis_Casualty_Company', 'Companion_Property_and_Casualty_Ins_Co', 'First_Financial_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Howard_Global', 'Merchants_Preferred_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Rockhill_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'SureTec_Insurance_Company', 'United_Fire_Casualty_Group', 'United_States_Fire_Insurance_Company', 'West_American_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 188): ['CT', 'DE', 'FL', 'HI', 'LA', 'MA', 'ME', 'MS', 'PR', 'RI', 'SC', 'VT', 'WV', 'AHG', 'CEQ', 'DF', 'EPLDO', 'EPLI', 'FT', 'GARAGE_PKG', 'PAF', 'PAP', 'PERQK', 'PHO', 'PWO', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_International_Group_AIG_', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Lloyds_of_London', 'Markel', 'Markel_Insurance_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'OneBeacon_Insurance_Co', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Specialty_Insurance_Co', 'Seneca_Specialty_Insurance_Company', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Tapco', 'Technology_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich', 'Zurich_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 188 | ['CT', 'DE', 'FL', 'HI', 'LA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 145 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 141 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t5.6s = Fit runtime\n",
      "\t145 features in original data used to generate 145 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 6.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3.18s of the 3.17s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.93\t = Validation score   (accuracy)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.35s of the 2.35s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1019.0 GB\n",
      "\t0.9068\t = Validation score   (accuracy)\n",
      "\t13.66s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 3.93s of the -11.80s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t0.9312\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 22.02s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 164075.9 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp95mnga3_\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmxm2sv3z\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.97 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmxm2sv3z\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043378.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 327 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 19): ['BindRequest', 'CEQ', 'FT', 'Chartis', 'Dallas_National_Insurance_Company', 'Founders_Ins_Co_Penn_National_', 'Great_American_Insurance_Group', 'Ironshore_Indemnity_Inc_', 'Lloyds_Canopius', 'Milwaukee_Casualty_Insurance_Company', 'Risk_Placement_Services_Inc_', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'United_National_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Zenith_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 182): ['WiringUpdate', 'BC', 'DE', 'LA', 'MA', 'ME', 'PR', 'VT', 'WV', 'AHG', 'BR', 'CPKG_AUTO', 'DF', 'EPLDO', 'GARAGE_PKG', 'PAF', 'PAP', 'PERQK', 'POLL', 'PWO', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis_Casualty_Company', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_of_London', 'Markel', 'McGowan', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Fire_Insurance_Co', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Contractors_Ins_Co_RRG_LLC', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Rockhill_Insurance_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'State_Auto_P_C_Ins_Co_', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'West_American_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zurich']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 182 | ['WiringUpdate', 'BC', 'DE', 'LA', 'MA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 149 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 145 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t5.1s = Fit runtime\n",
      "\t149 features in original data used to generate 149 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9348\t = Validation score   (accuracy)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3.31s of the 3.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9376\t = Validation score   (accuracy)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.37s of the 2.37s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9064\t = Validation score   (accuracy)\n",
      "\t12.44s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -10.57s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t0.9376\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.76s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 332596.2 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmxm2sv3z\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpx_xxn1ua\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.97 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpx_xxn1ua\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043345.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 337 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 9): ['BindRequest', '_CNA_Transportation_Insurance_Company', 'Atrium_via_Ambris_LLLP', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Professional_Solutions_Insurance_Co_', 'ROCHDALE_INSURANCE_CO', 'Safeco_Ins_Co_of_America_Safeco_', 'Unitrin_County_Mutual_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 195): ['WiringUpdate', 'BC', 'MA', 'ME', 'NC', 'PR', 'RI', 'SC', 'VT', 'WV', 'AHG', 'CEQ', 'DF', 'EPLDO', 'FT', 'PAP', 'PERQK', 'POLL', 'PWO', 'TL', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', 'AIG', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_International_Group_AIG_', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_', 'C_N_A_Surety', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'Other', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Penn_Nat_Mut_Cas_Ins_Co_Penn_National_', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'Progressive_Insurance_Company', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Co', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'WESCO_Insurance_Company', 'West_American_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 195 | ['WiringUpdate', 'BC', 'MA', 'ME', 'NC', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 146 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 142 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t4.9s = Fit runtime\n",
      "\t146 features in original data used to generate 146 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.72s of the 4.71s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9244\t = Validation score   (accuracy)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3.45s of the 3.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9256\t = Validation score   (accuracy)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.49s of the 2.49s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9044\t = Validation score   (accuracy)\n",
      "\t15.29s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.72s of the -13.36s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.875, 'LightGBMXT': 0.125}\n",
      "\t0.926\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 23.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 185113.6 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpx_xxn1ua\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5s608h6\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5s608h6\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043362.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 333 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 13): ['BindRequest', 'AXIS_Insurance_Company', 'Admiral_Insurance_Company', 'Allied', 'Burlington_Insurance_Co', 'Carolina_Casualty_Insurance_Co', 'Empire', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'Fitchburg_Mutual_Insurance_Co_', 'Mount_Vernon_Insurance_Company', 'State_Auto_P_C_Ins_Co_', 'Tudor_Insurance_Co', 'United_Specialty_Insurance_Co_']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 192): ['BC', 'DE', 'HI', 'LA', 'MA', 'ME', 'SC', 'VT', 'BR', 'CEQ', 'CPKG_AUTO', 'CYB', 'EPLDO', 'FT', 'MTC', 'PAF', 'PAP', 'PERQK', 'POLL', 'PWO', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'C_N_A_', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Explorer_Insurance_Company', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'Ironshore_Indemnity_Inc_', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Agency_Underwriters', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Lloyds_of_London', 'Markel', 'Markel_Insurance_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Fire_Insurance_Co', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'West_American_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 192 | ['BC', 'DE', 'HI', 'LA', 'MA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 145 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 141 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t5.3s = Fit runtime\n",
      "\t145 features in original data used to generate 145 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9224\t = Validation score   (accuracy)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3.28s of the 3.27s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9196\t = Validation score   (accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.35s of the 2.35s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.8988\t = Validation score   (accuracy)\n",
      "\t12.18s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.27s of the -10.32s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9224\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.48s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 357010.7 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5s608h6\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmphoy8imi4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmphoy8imi4\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043337.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 325 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 21): ['BindRequest', 'PAP', 'XEO', 'Associated_Industries_Insurance_Co_Inc', 'C_N_A_Surety', 'Commerce_West_Insurance_Company', 'Crum_Forster_Insurance', 'Explorer_Insurance_Company', 'Granite_State_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Metcom_Excess', 'Mt_Hawley_Insurance_Company', 'Norman_Spencer', 'RSUI_Indemnity_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Twin_City_Fire_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'Western_World']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 192): ['BC', 'CT', 'DE', 'FL', 'HI', 'MA', 'ME', 'MS', 'NC', 'ND', 'PR', 'SC', 'VT', 'WV', 'AHG', 'BR', 'CEQ', 'CPKG_AUTO', 'DF', 'EPLDO', 'FT', 'GARAGE_PKG', 'PAF', 'PERQK', 'POLL', 'PWO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Hartford', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'McGowan', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_Nat_Mut_Cas_Ins_Co_Penn_National_', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Contractors_Ins_Co_RRG_LLC', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Casualty_Inc_Co_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'US_Underwriters_Insurance_Co', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 192 | ['BC', 'CT', 'DE', 'FL', 'HI', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 137 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 133 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t4.8s = Fit runtime\n",
      "\t137 features in original data used to generate 137 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 20.40 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.85s of the 4.85s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9268\t = Validation score   (accuracy)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9252\t = Validation score   (accuracy)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.59s of the 2.58s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.906\t = Validation score   (accuracy)\n",
      "\t14.31s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.85s of the -12.20s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9268\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 22.39s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 382245.6 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmphoy8imi4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcwyutw5u\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcwyutw5u\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043366.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 329 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 17): ['BindRequest', 'BC', 'AmTrust', 'American_Guarantee', 'Chartis_Casualty_Company', 'Companion_Property_and_Casualty_Ins_Co', 'First_Financial_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Howard_Global', 'Merchants_Preferred_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Rockhill_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'SureTec_Insurance_Company', 'United_Fire_Casualty_Group', 'United_States_Fire_Insurance_Company', 'West_American_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 188): ['CT', 'DE', 'FL', 'HI', 'LA', 'MA', 'ME', 'MS', 'PR', 'RI', 'SC', 'VT', 'WV', 'AHG', 'CEQ', 'DF', 'EPLDO', 'EPLI', 'FT', 'GARAGE_PKG', 'PAF', 'PAP', 'PERQK', 'PHO', 'PWO', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_International_Group_AIG_', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Lloyds_of_London', 'Markel', 'Markel_Insurance_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'OneBeacon_Insurance_Co', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Specialty_Insurance_Co', 'Seneca_Specialty_Insurance_Company', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Tapco', 'Technology_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich', 'Zurich_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 188 | ['CT', 'DE', 'FL', 'HI', 'LA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 145 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 141 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t6.7s = Fit runtime\n",
      "\t145 features in original data used to generate 145 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 7.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 2.75s of the 2.75s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.88s of the 1.88s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.93\t = Validation score   (accuracy)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1.07s of the 1.06s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9068\t = Validation score   (accuracy)\n",
      "\t11.83s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2.75s of the -11.30s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t0.9312\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 191801.0 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcwyutw5u\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp0onmo2d4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp0onmo2d4\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043332.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 327 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 19): ['BindRequest', 'CEQ', 'FT', 'Chartis', 'Dallas_National_Insurance_Company', 'Founders_Ins_Co_Penn_National_', 'Great_American_Insurance_Group', 'Ironshore_Indemnity_Inc_', 'Lloyds_Canopius', 'Milwaukee_Casualty_Insurance_Company', 'Risk_Placement_Services_Inc_', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'United_National_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Zenith_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 182): ['WiringUpdate', 'BC', 'DE', 'LA', 'MA', 'ME', 'PR', 'VT', 'WV', 'AHG', 'BR', 'CPKG_AUTO', 'DF', 'EPLDO', 'GARAGE_PKG', 'PAF', 'PAP', 'PERQK', 'POLL', 'PWO', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AIG', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Argonaut_Insurance_Company', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis_Casualty_Company', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Fitchburg_Mutual_Insurance_Co_', 'Foremost_Insurance_Group', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_of_London', 'Markel', 'McGowan', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Fire_Insurance_Co', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Contractors_Ins_Co_RRG_LLC', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Rockhill_Insurance_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'State_Auto_P_C_Ins_Co_', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Insurance_Company', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'West_American_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zurich']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 182 | ['WiringUpdate', 'BC', 'DE', 'LA', 'MA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 149 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 145 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t5.4s = Fit runtime\n",
      "\t149 features in original data used to generate 149 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.87s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9348\t = Validation score   (accuracy)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3.04s of the 3.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9376\t = Validation score   (accuracy)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 2.18s of the 2.18s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9064\t = Validation score   (accuracy)\n",
      "\t15.47s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.13s of the -13.79s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t0.9376\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 23.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 350377.9 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp0onmo2d4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_pst2apx\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_pst2apx\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043358.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 337 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 9): ['BindRequest', '_CNA_Transportation_Insurance_Company', 'Atrium_via_Ambris_LLLP', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Professional_Solutions_Insurance_Co_', 'ROCHDALE_INSURANCE_CO', 'Safeco_Ins_Co_of_America_Safeco_', 'Unitrin_County_Mutual_Insurance_Company']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 195): ['WiringUpdate', 'BC', 'MA', 'ME', 'NC', 'PR', 'RI', 'SC', 'VT', 'WV', 'AHG', 'CEQ', 'DF', 'EPLDO', 'FT', 'PAP', 'PERQK', 'POLL', 'PWO', 'TL', 'USTL', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', 'AIG', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'AXIS_Insurance_Company', 'Acceptance_Casualty_Co_', 'Admiral_Insurance_Company', 'Allianz_Global_Corporate_Specialty', 'Allied', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_International_Group_AIG_', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Argonaut_Insurance_Company', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'Burlington_Insurance_Co', 'C_N_A_', 'C_N_A_Surety', 'CFC_Underwriting', 'CNA', 'Canopius_US_Insurance_Inc_', 'Carolina_Casualty_Insurance_Co', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Empire', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Essex_Insurance_Co', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Excelsior_Insurance_Co_Peerless_', 'Explorer_Insurance_Company', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Golden_Bear_Management_Corporation', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Houston_Specialty_Insurance_Co', 'Howard_Global', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Ironshore_Indemnity_Inc_', 'K_K_INSURANCE_GROUP', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Markel', 'Markel_Insurance_Company', 'Maxum_Indemnity_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Insurance_Company', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Fire_and_Marine_Insurance_Co', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'Other', 'Palomar_Specialty_Insurance_Company', 'Peerless_Indemnity_Ins_Co_Peerless_', 'Penn_Nat_Mut_Cas_Ins_Co_Penn_National_', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Progressive_Auto_Insurance_Company', 'Progressive_Casualty_Insurance_Company', 'Progressive_Insurance_Company', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Rockingham_Casualty_Company', 'Safeco', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Co', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'State_Auto_P_C_Ins_Co_', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Tudor_Insurance_Co', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_Specialty_Insurance_Co_', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'WESCO_Insurance_Company', 'West_American_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company', 'Zurich_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 195 | ['WiringUpdate', 'BC', 'MA', 'ME', 'NC', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 146 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 142 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t6.2s = Fit runtime\n",
      "\t146 features in original data used to generate 146 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.52 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 6.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.40s of the 3.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9244\t = Validation score   (accuracy)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.03s of the 2.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9256\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1.11s of the 1.11s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9044\t = Validation score   (accuracy)\n",
      "\t11.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 3.40s of the -11.22s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.875, 'LightGBMXT': 0.125}\n",
      "\t0.926\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21.39s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 178535.7 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_pst2apx\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp3qd5o3ok\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1018.96 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp3qd5o3ok\"\n",
      "Train Data Rows:    129672\n",
      "Train Data Columns: 350\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043342.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 346.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 333 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 13): ['BindRequest', 'AXIS_Insurance_Company', 'Admiral_Insurance_Company', 'Allied', 'Burlington_Insurance_Co', 'Carolina_Casualty_Insurance_Co', 'Empire', 'Fid_Guar_Ins_Undwrtrs_Travelers_', 'Fitchburg_Mutual_Insurance_Co_', 'Mount_Vernon_Insurance_Company', 'State_Auto_P_C_Ins_Co_', 'Tudor_Insurance_Co', 'United_Specialty_Insurance_Co_']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 192): ['BC', 'DE', 'HI', 'LA', 'MA', 'ME', 'SC', 'VT', 'BR', 'CEQ', 'CPKG_AUTO', 'CYB', 'EPLDO', 'FT', 'MTC', 'PAF', 'PAP', 'PERQK', 'POLL', 'PWO', 'XEO', '_CNA_Valley_Forge_Insurance_Company', '_CNA_National_Fire_Ins_Co_of_Hartford', '_CNA_Transportation_Insurance_Company', 'AON_Insurance', 'ATRIUM_Certain_Underwriters_at_Lloyd_s', 'Acceptance_Casualty_Co_', 'Allianz_Global_Corporate_Specialty', 'Allied_World_National_Insurance_Co_', 'AmTrust', 'AmTrust_Insurance_Company_of_Kansas', 'AmTrust_International_Underwriters_Ltd', 'AmTrust_North_America_Inc_L_H_', 'American_Alternative_Insurance_Corporation', 'American_Casualty_Company_of_Reading_Pennsylvania', 'American_Guarantee', 'American_Safety_Casualty_Insurance_Company', 'Amtrust_International_Underwriters_Limited', 'Ark_via_Ambris_LLLP', 'Associated_Industries_Insurance_Co_Inc', 'Atain_Specialty_Insurance_Company', 'Atlantic_Casualty_Insurance_Co_', 'Atrium_via_Ambris_LLLP', 'Bankers_Insurance_Group', 'Berkley_Assurance_Co', 'Berkshire_Hathaway_Homestate_Insurance_Co', 'C_N_A_', 'C_N_A_Surety', 'CFC_Underwriting', 'CHUBB', 'CNA', 'Canopius_US_Insurance_Inc_', 'Century_Surety_Company', 'Certain_Underwriters_at_Lloyd_s', 'Certain_Underwriters_at_Lloyd_s_of_London_B0595', 'Chartis', 'Chartis_Casualty_Company', 'Colony_Insurance_Co', 'Commerce_West_Insurance_Company', 'Companion_Property_and_Casualty_Ins_Co', 'Continental_Casualty', 'Covington_Specialty_Insurance_Company', 'Crum_Forster_Insurance', 'Cumberland_Mut_Fire_Ins_Co_Cumberland_', 'Dallas_National_Insurance_Company', 'Employers_Compensation_Insurance_Company', 'Endurance_American_Insurance_Company', 'Endurance_Contract_Binding', 'Essex_Insurance_Co', 'Essex_Insurance_Company', 'Evanston_Insurance_Company', 'Everest_National_Insurance_Company', 'Explorer_Insurance_Company', 'First_Financial_Insurance_Company', 'FirstComp_Insurance_Company', 'Foremost_Insurance_Group', 'Founders_Ins_Co_Penn_National_', 'Franklin_Mutual_Insurance_Company_FMI_', 'Gemini_Insurance_Company', 'GeoVera_Insurance_Company', 'GeoVera_Specialty_Insurance_Company', 'Golden_Bear_Insurance_Company', 'Granite_State_Insurance_Company', 'Great_American_Insurance_Group', 'Great_American_Insurance_Group_of_NY', 'Greenwich_Insurance_Co_', 'Guard_Insurance_Group', 'Hartford_Casualty_Insurance', 'Houston_Casualty_Co_', 'Howard_Global', 'Hudson_Insurance_Company', 'Indian_Harbor_Insurance_Company', 'Infinity_Auto_Insurance_Company', 'Insurance_Company_of_the_West', 'Intl_Ins_Co_of_Hannover', 'Ironshore_Indemnity_Inc_', 'Leading_Insurance_Group_Ins_Co_Ltd_USB', 'Lexington_Insurance_Company', 'Liberty_Agency_Underwriters', 'Liberty_Insurance_Underwriters', 'Liberty_Mutual', 'Lloyd_s', 'Lloyd_s_Underwriters_PPIB_', 'Lloyd_s_Underwriters_WKF_C_', 'Lloyd_s_of_London', 'Lloyds_Canopius', 'Lloyds_of_London', 'Markel', 'Markel_Insurance_Company', 'Merchants_Mutual_Ins_Co_Merchants_', 'Merchants_Preferred_Insurance_Company', 'Merrimack_Mutual_Fire_Ins_Co_Andover_', 'Mesa_Underwriters_Specialty_Ins_Co_', 'Metcom_Excess', 'Milwaukee_Casualty_Insurance_Company', 'Missouri_Employers_Mutual_MEM_', 'Mount_Vernon_Fire_Insurance_Co', 'Mt_Hawley_Insurance_Company', 'National_Fire', 'National_Union_Fire_Ins_Co_Pittsburgh_PA', 'Nautilus_Insurance', 'NorGuard_Insurance_Company', 'Norman_Spencer', 'Northfield_Insurance_Company', 'Ohio_Casualty_Group', 'Ohio_Security_Insurance_Company', 'OneBeacon_Insurance_Co', 'Other', 'Peerless_Insurance_Company', 'Penn_America_Insurance_Company', 'Philadelphia_Insurance_Companies', 'Philadelphia_Insurance_Company', 'Plaza_Insurance_Company_State_Auto_', 'Preferred_Mutual_Ins_Co_Preferred_', 'Professional_Solutions_Insurance_Co_', 'Progressive_Auto_Insurance_Company', 'QBE', 'ROCHDALE_INSURANCE_CO', 'RSUI_Indemnity_Company', 'Risk_Placement_Services_Inc_', 'Rockhill_Insurance_Company', 'Safeco', 'Safeco_Ins_Co_of_America_Safeco_', 'Safeco_Ins_co_of_Illinois_Safeco_', 'Scottsdale_Insurance_Company', 'Selective_Auto_Ins_Co_of_NJ_Selective_', 'Selective_Casualty_Inc_Co_', 'Selective_Fire_and_Casualty_Ins_Co_', 'Selective_Ins_Co_of_America_Selective_', 'Selective_Way_Ins_Co_Selective_', 'Seneca_Insurance_Company', 'Seneca_Insurance_Company_Inc', 'Seneca_Specialty_Insurance_Company', 'Sentinel_Insurance_Company_Limited', 'Starr_Indemnity_STARR_', 'State_Auto_Mutual_Ins_Co', 'Stillwater_Insurance_Co_Fidelity_', 'Stillwater_Insurance_Group', 'SureTec_Insurance_Company', 'Tapco', 'Technology_Insurance_Company', 'Torus_National_Insurance_Co', 'Torus_National_Insurance_Company', 'Torus_Specialty_Insurance_Co', 'Tower_Ins_Co_of_New_York_Tower_', 'Tower_Insurance_Companies', 'Travelers_Auto_Ins_Co_of_NJ_Travelers_', 'Travelers_Casualty_Insurance_Company_of_America', 'Travelers_Indemnity_Company_Travelers_', 'Travelers_Indemnity_Company_of_America', 'Travelers_Prop_Cas_Ins_Co_Travelers_', 'Travelers_Prop_Insurance_Company', 'Twin_City_Fire_Insurance_Co', 'U_S_Underwriters_Insurance_Company', 'US_Underwriters_Insurance_Co', 'Underwriters_at_Lloyd_s_London_Illinois_', 'United_Fire_Casualty_Group', 'United_National_Insurance_Company', 'United_Property_Casualty_Insurance_Co', 'United_Specialty_Insurance_Co', 'United_Specialty_Insurance_Co_CS', 'United_States_Fidelity_and_Guaranty_Company', 'United_States_Fire_Insurance_Company', 'Unitrin_County_Mutual_Insurance_Company', 'WESCO_Insurance_Company', 'West_American_Insurance_Company', 'Westchester_Fire_Insurance_Company', 'Western_Heritage_Insurance_Company', 'Western_Surety_Co_', 'Western_World', 'Western_World_Insurance_Co', 'Zenith_Insurance_Company']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 192 | ['BC', 'DE', 'HI', 'LA', 'MA', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 145 | ['Unnamed_0', 'IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   4 | ['Unnamed_0', 'YearTerm', 'EffectiveYear', 'ExpirationYear']\n",
      "\t\t('int', ['bool']) : 141 | ['IsSurplusLines', 'IsAgencyBilled', 'ProducerHoldFlag', 'BinderExpectedFlag', 'IsRenewal', ...]\n",
      "\t5.4s = Fit runtime\n",
      "\t145 features in original data used to generate 145 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.99s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.019279412671972362, Train Rows: 127172, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.01s of the 4.00s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9224\t = Validation score   (accuracy)\n",
      "\t1.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.11s of the 2.11s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.9196\t = Validation score   (accuracy)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1.22s of the 1.21s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.1/1018.9 GB\n",
      "\t0.8988\t = Validation score   (accuracy)\n",
      "\t12.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.01s of the -11.54s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9224\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 22.11s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 356331.3 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp3qd5o3ok\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkxryqy84\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.19 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkxryqy84\"\n",
      "Train Data Rows:    712\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043642.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 569, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.93s of the 9.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.59s of the 9.58s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.40s of the 8.40s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t1.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 6.65s of the 6.65s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 5.99s of the 5.98s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t5.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 0.74s of the 0.74s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.93s of the 0.03s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.05s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2331.3 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkxryqy84\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpeokvawb0\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpeokvawb0\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043632.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7832\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.37s of the 9.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.86s of the 8.86s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.15s of the 8.15s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.45s of the 7.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\tRan out of time, early stopping on iteration 348.\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t7.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the -0.01s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35136.8 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpeokvawb0\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1xrnk1fg\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1xrnk1fg\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043632.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.52s of the 9.52s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.04s of the 9.04s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.34s of the 8.34s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.61s of the 7.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t5.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 2.31s of the 2.31s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1.60s of the 1.60s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 0.90s of the 0.89s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.8392\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 0.18s of the 0.18s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the -0.03s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.13s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35748.3 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1xrnk1fg\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmppw3a9349\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmppw3a9349\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043630.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.06s of the 9.05s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8112\t = Validation score   (accuracy)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.30s of the 8.29s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8252\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.65s of the 7.65s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.01s of the 7.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\tRan out of time, early stopping on iteration 314.\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t6.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the 0.00s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.06s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35744.1 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmppw3a9349\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp42ph9za9\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp42ph9za9\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043629.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.42s of the 9.42s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.81s of the 8.81s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7692\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.09s of the 8.09s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7832\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.37s of the 7.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t4.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 2.85s of the 2.85s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7692\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 2.14s of the 2.14s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7622\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1.42s of the 1.41s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 0.31s of the 0.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 0.11s of the 0.11s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\t0.6154\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the -0.01s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.318, 'LightGBM': 0.136, 'CatBoost': 0.045}\n",
      "\t0.8392\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.12s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5268.8 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp42ph9za9\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7qxhr4u\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7qxhr4u\"\n",
      "Train Data Rows:    712\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043628.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 569, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.95s of the 9.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.60s of the 8.60s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 7.31s of the 7.30s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 6.50s of the 6.50s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 5.83s of the 5.82s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t5.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 0.62s of the 0.61s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.95s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t0.8741\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2253.9 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7qxhr4u\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp6use1slp\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp6use1slp\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043627.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7832\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.40s of the 9.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.90s of the 8.90s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.19s of the 8.19s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.49s of the 7.49s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\tRan out of time, early stopping on iteration 340.\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t7.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the -0.00s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35737.7 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp6use1slp\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpj03meud4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpj03meud4\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043627.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.94s of the 9.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.52s of the 9.52s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.02s of the 9.02s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.32s of the 8.32s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.61s of the 7.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t5.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 2.18s of the 2.18s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8462\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1.48s of the 1.48s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8531\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 0.78s of the 0.78s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.8392\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.94s of the 0.06s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8671\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.03s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 35741.9 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpj03meud4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpt53mcd7u\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpt53mcd7u\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043628.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.93s of the 9.93s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8322\t = Validation score   (accuracy)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.37s of the 8.36s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.8112\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 7.97s of the 7.96s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8252\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.22s of the 7.21s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8182\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 6.50s of the 6.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\tRan out of time, early stopping on iteration 411.\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t6.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.93s of the -0.01s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8601\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 28563.9 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpt53mcd7u\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpplg76mrl\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpplg76mrl\"\n",
      "Train Data Rows:    713\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043628.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['Name']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['Name']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])    : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['Ticket', 'Cabin', 'Embarked']\n",
      "\t\t('float', [])     : 6 | ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.0s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 570, Val Rows: 143\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.95s of the 9.95s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7902\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.61s of the 9.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.21s of the 9.21s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7692\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.46s of the 8.46s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7832\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.73s of the 7.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t4.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 2.78s of the 2.78s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7692\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 2.06s of the 2.06s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7622\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1.35s of the 1.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.8042\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 0.78s of the 0.77s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.7972\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 0.55s of the 0.55s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\t0.7413\t = Validation score   (accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.95s of the 0.02s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.6, 'CatBoost': 0.2, 'LightGBM': 0.1, 'NeuralNetTorch': 0.1}\n",
      "\t0.8252\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.1s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4591.1 rows/s (143 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (143 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpplg76mrl\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpylydozla\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.17 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpylydozla\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043627.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.23s of the 9.23s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.94s of the 8.94s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.2 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.64s of the 8.64s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.92s of the 7.92s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.23s of the 7.23s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.95s of the 6.95s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.23s of the 6.23s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.56s of the 5.56s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.44s of the 5.44s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.72s of the 4.72s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.27s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.5}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2617.7 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpylydozla\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptkv7xxw9\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmptkv7xxw9\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043855.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.33s of the 9.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.07s of the 9.07s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.81s of the 8.81s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.10s of the 8.10s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.47s of the 7.47s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.19s of the 7.19s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.55s of the 6.55s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.92s of the 5.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.78s of the 5.78s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.09s of the 5.09s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.74s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.36s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 489.5 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptkv7xxw9\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpnn51j1ed\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpnn51j1ed\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043855.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.35s of the 9.35s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.10s of the 9.10s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.84s of the 8.84s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.16s of the 8.16s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.50s of the 7.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.23s of the 7.23s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.58s of the 6.58s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.91s of the 5.91s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.79s of the 5.79s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.92s of the 4.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.62s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.53s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3113.5 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpnn51j1ed\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpn25lo_oa\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpn25lo_oa\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043855.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.31s of the 9.30s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.08s of the 9.08s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.78s of the 8.78s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.12s of the 8.11s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.39s of the 7.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.13s of the 7.12s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.42s of the 6.42s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.74s of the 5.74s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.59s of the 5.59s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.91s of the 4.91s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.55s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.55s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 497.2 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpn25lo_oa\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptniew3j2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmptniew3j2\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.29s of the 9.29s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.03s of the 9.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.77s of the 8.77s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.12s of the 8.12s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.48s of the 7.48s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.19s of the 7.19s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.50s of the 6.50s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.88s of the 5.88s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.69s of the 5.69s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.08s of the 5.08s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.69s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.42s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3982.4 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptniew3j2\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgdos7e11\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.22 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgdos7e11\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.35s of the 9.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.08s of the 9.08s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.75s of the 8.75s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.03s of the 8.03s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.30s of the 7.30s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.07s of the 7.07s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.38s of the 6.38s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.67s of the 5.67s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.53s of the 5.53s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.76s of the 4.76s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.35s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.5}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.76s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2399.6 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgdos7e11\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5ysoa1h\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.20 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5ysoa1h\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.32s of the 9.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.99s of the 8.99s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.69s of the 8.68s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.03s of the 8.02s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.34s of the 7.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.10s of the 7.10s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.45s of the 6.45s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.75s of the 5.75s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.63s of the 5.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.92s of the 4.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.53s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.6s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 389.5 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpq5ysoa1h\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkuuttxso\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkuuttxso\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043855.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.34s of the 9.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.01s of the 9.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.67s of the 8.67s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.01s of the 8.01s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.34s of the 7.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.05s of the 7.05s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.37s of the 6.37s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.69s of the 5.69s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.57s of the 5.57s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.70s of the 4.70s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.38s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.73s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3999.0 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkuuttxso\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmps2dgnnxr\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmps2dgnnxr\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.39s of the 9.39s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.14s of the 9.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.87s of the 8.86s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.18s of the 8.18s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.49s of the 7.48s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.22s of the 7.22s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.48s of the 6.48s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.73s of the 5.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.61s of the 5.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.03s of the 5.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.59s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.51s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 373.2 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmps2dgnnxr\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_zf8ci1\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_zf8ci1\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.34s of the 9.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.08s of the 9.08s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.82s of the 8.81s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.13s of the 8.13s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.43s of the 7.43s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.18s of the 7.18s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.55s of the 6.55s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.96s of the 5.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.85s of the 5.85s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.13s of the 5.13s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 4.78s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.33s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3999.3 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp_zf8ci1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpujoa9ic3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpujoa9ic3\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.28s of the 9.28s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.01s of the 9.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.68s of the 8.67s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.96s of the 7.96s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.31s of the 7.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.85s of the 6.85s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.17s of the 6.16s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.49s of the 5.49s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.37s of the 5.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.06s of the 5.06s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.66s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 604.0 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpujoa9ic3\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxhkc60ag\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxhkc60ag\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.31s of the 9.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.01s of the 9.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.69s of the 8.68s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.06s of the 8.06s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.41s of the 7.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.91s of the 6.90s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.21s of the 6.21s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.53s of the 5.53s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.37s of the 5.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.96s of the 4.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.54s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5303.0 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxhkc60ag\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpe6zbyftu\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpe6zbyftu\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.95s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.15s of the 9.15s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.87s of the 8.86s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.61s of the 8.60s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.88s of the 7.88s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.21s of the 7.20s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.74s of the 6.74s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.01s of the 6.00s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.31s of the 5.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.14s of the 5.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.70s of the 4.70s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.29s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.82s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4832.3 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpe6zbyftu\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_p0gacvc\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   106.21 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_p0gacvc\"\n",
      "Train Data Rows:    143\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043854.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 114, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.32s of the 9.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.07s of the 9.07s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.81s of the 8.81s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.17s of the 8.17s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.48s of the 7.48s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.01s of the 7.01s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.39s of the 6.39s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.73s of the 5.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.54s of the 5.54s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.63s of the 4.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.01s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.1s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 585.7 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp_p0gacvc\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp2604pg69\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp2604pg69\"\n",
      "Train Data Rows:    143\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 114, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.27s of the 9.26s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.97s of the 8.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.64s of the 8.64s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.96s of the 7.96s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.31s of the 7.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.84s of the 6.83s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.17s of the 6.17s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.52s of the 5.52s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.38s of the 5.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.90s of the 4.89s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.09s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4832.3 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp2604pg69\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpew2efar5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpew2efar5\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.33s of the 9.33s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.03s of the 9.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.72s of the 8.72s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.08s of the 8.08s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.42s of the 7.42s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.95s of the 6.94s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.30s of the 6.30s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.63s of the 5.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.45s of the 5.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.13s of the 5.13s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.30s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.81s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 603.5 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpew2efar5\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkiv4w193\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkiv4w193\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.95s of the 9.95s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.32s of the 9.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.03s of the 9.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.75s of the 8.75s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.07s of the 8.07s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.42s of the 7.42s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.94s of the 6.94s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.27s of the 6.27s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.59s of the 5.58s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.38s of the 5.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.01s of the 5.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.95s of the 4.41s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.69s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 9663.5 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpkiv4w193\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmqj0mj45\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmqj0mj45\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.17s of the 9.17s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.81s of the 8.81s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.48s of the 8.48s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.84s of the 7.84s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.20s of the 7.20s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.71s of the 6.71s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.01s of the 6.01s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.32s of the 5.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.14s of the 5.13s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.73s of the 4.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 4.19s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.92s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4831.8 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpmqj0mj45\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpsx97y5uu\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpsx97y5uu\"\n",
      "Train Data Rows:    143\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 114, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.31s of the 9.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.01s of the 9.01s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.69s of the 8.69s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.03s of the 8.03s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.35s of the 7.35s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.87s of the 6.87s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.21s of the 6.21s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.56s of the 5.56s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.40s of the 5.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.71s of the 4.70s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 3.88s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.23s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 606.0 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpsx97y5uu\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpll_qd3m2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       1019.39 GB / 1024.00 GB (99.5%)\n",
      "Disk Space Avail:   105.99 GB / 476.13 GB (22.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Enforcing custom memory (soft) limit of 1024 GB!\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpll_qd3m2\"\n",
      "Train Data Rows:    143\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1043853.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['Alcohol', 'Malic_acid', 'Ash', 'Acl', 'Mg', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 114, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "No improvement since epoch 5: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.27s of the 9.27s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 8.96s of the 8.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.64s of the 8.64s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 7.99s of the 7.98s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.32s of the 7.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 6.82s of the 6.82s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.18s of the 6.18s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.50s of the 5.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.31s of the 5.31s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.9655\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.71s of the 4.71s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/1019.4 GB\n",
      "\t0.931\t = Validation score   (accuracy)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 3.93s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7248.4 rows/s (29 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpll_qd3m2\")\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Task 3+9 ----------------\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import torch\n",
    "\n",
    "def evaluate_cv_folds_autogluon(folds_dict):\n",
    "    for dataset_name, folds_data in folds_dict.items():\n",
    "        task_type = tasks_dict_classification_only[dataset_name]\n",
    "        target_col = TARGET_COLS[dataset_name]\n",
    "\n",
    "        dataset_summary = []\n",
    "        runtime_memory_rows = []\n",
    "        dataset_base = dataset_name.replace(\".csv\", \"\")\n",
    "\n",
    "        for seed in RANDOM_SEEDS:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            try:\n",
    "                torch.manual_seed(seed)\n",
    "                torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            fold_num = 1\n",
    "            for fold_name, fold_splits in folds_data.items():\n",
    "                df_train = fold_splits['train']\n",
    "                df_val   = fold_splits['val']\n",
    "\n",
    "                X_val = df_val.drop(columns=[target_col])\n",
    "                y_val = df_val[target_col]\n",
    "\n",
    "                temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "                start_time = time.time()\n",
    "                predictor = TabularPredictor(\n",
    "                    label=target_col,\n",
    "                    problem_type=task_type,\n",
    "                    path=temp_dir\n",
    "                ).fit(\n",
    "                    df_train,\n",
    "                    time_limit=CV_TIME_BUDGET,\n",
    "                    memory_limit=CV_MEMORY_LIMIT\n",
    "                )\n",
    "                training_runtime_sec = round(time.time() - start_time, 3)\n",
    "\n",
    "                infer_start = time.time()\n",
    "                preds = predictor.predict(X_val)\n",
    "                results = evaluate_and_save_results_general(df_train, df_val, predictor, dataset_name, framework='autogluon')\n",
    "                inference_time_per_sample = round((time.time() - infer_start) / len(X_val), 6)\n",
    "                cpu_usage_percent = psutil.cpu_percent(interval=None)\n",
    "\n",
    "                total_bytes = 0\n",
    "                for root, dirs, files in os.walk(temp_dir):\n",
    "                    for f in files:\n",
    "                        total_bytes += os.path.getsize(os.path.join(root, f))\n",
    "                model_size_mb = round(total_bytes / (1024 * 1024), 3)\n",
    "\n",
    "                memory_usage_mb = get_memory_usage_mb()\n",
    "\n",
    "                dataset_summary.append({\n",
    "                    'dataset': dataset_name,\n",
    "                    'seed': seed,\n",
    "                    'fold': fold_num,\n",
    "                    **results,\n",
    "                    'runtime': training_runtime_sec\n",
    "                })\n",
    "\n",
    "                runtime_memory_rows.append({\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_num,\n",
    "                    \"training_runtime_sec\": training_runtime_sec,\n",
    "                    \"model_size_MB\": model_size_mb,\n",
    "                    \"memory_usage_MB\": memory_usage_mb,\n",
    "                    \"inference_time_per_sample_sec\": inference_time_per_sample,\n",
    "                    \"cpu_usage_percent\": cpu_usage_percent\n",
    "                })\n",
    "\n",
    "                fold_num += 1\n",
    "                shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "                del predictor\n",
    "                gc.collect()\n",
    "\n",
    "        output_path = os.path.join(CV_FOLDS_METRICS_PATH, f\"autogluon_{dataset_base}_cv_summary.csv\")\n",
    "        pd.DataFrame(dataset_summary).to_csv(output_path, index=False)\n",
    "\n",
    "        task3_output_path = os.path.join(RUNTIME_MEMORY_TASK3_PATH, f\"autogluon_{dataset_base}_task3_runtime_memory_cv_metrics.csv\")\n",
    "        pd.DataFrame(runtime_memory_rows).to_csv(task3_output_path, index=False)\n",
    "\n",
    "        json_path = os.path.join(RUNTIME_MEMORY_TASK3_PATH, f\"autogluon_{dataset_base}_runtime_memory_metrics_cv_in_json.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(runtime_memory_rows, f, indent=2)\n",
    "\n",
    "evaluate_cv_folds_autogluon(folds_dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daffa91b-abf4-4223-9afd-e9916947c185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>0.925368</td>\n",
       "      <td>0.925331</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>21.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.924055</td>\n",
       "      <td>0.923964</td>\n",
       "      <td>0.923901</td>\n",
       "      <td>0.924055</td>\n",
       "      <td>22.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.927263</td>\n",
       "      <td>0.927413</td>\n",
       "      <td>0.927654</td>\n",
       "      <td>0.927263</td>\n",
       "      <td>21.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>0.924380</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>24.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>0.929859</td>\n",
       "      <td>0.929836</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>21.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>0.925368</td>\n",
       "      <td>0.925331</td>\n",
       "      <td>0.925412</td>\n",
       "      <td>23.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.924055</td>\n",
       "      <td>0.923964</td>\n",
       "      <td>0.923901</td>\n",
       "      <td>0.924055</td>\n",
       "      <td>22.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.927263</td>\n",
       "      <td>0.927413</td>\n",
       "      <td>0.927654</td>\n",
       "      <td>0.927263</td>\n",
       "      <td>24.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>0.924380</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>22.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>0.929859</td>\n",
       "      <td>0.929836</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>22.822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  fold  accuracy        f1  precision    recall  runtime\n",
       "0  modeldata.csv    42     1  0.925412  0.925368   0.925331  0.925412   21.989\n",
       "1  modeldata.csv    42     2  0.924055  0.923964   0.923901  0.924055   22.841\n",
       "2  modeldata.csv    42     3  0.927263  0.927413   0.927654  0.927263   21.609\n",
       "3  modeldata.csv    42     4  0.923900  0.924081   0.924380  0.923900   24.231\n",
       "4  modeldata.csv    42     5  0.929885  0.929859   0.929836  0.929885   21.245\n",
       "5  modeldata.csv   123     1  0.925412  0.925368   0.925331  0.925412   23.212\n",
       "6  modeldata.csv   123     2  0.924055  0.923964   0.923901  0.924055   22.261\n",
       "7  modeldata.csv   123     3  0.927263  0.927413   0.927654  0.927263   24.715\n",
       "8  modeldata.csv   123     4  0.923900  0.924081   0.924380  0.923900   22.113\n",
       "9  modeldata.csv   123     5  0.929885  0.929859   0.929836  0.929885   22.822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_task3_runtime_memory_cv_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>21.989</td>\n",
       "      <td>559.316</td>\n",
       "      <td>5150.520</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>22.841</td>\n",
       "      <td>558.059</td>\n",
       "      <td>5151.535</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>91.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>21.609</td>\n",
       "      <td>563.095</td>\n",
       "      <td>5153.730</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>96.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>24.231</td>\n",
       "      <td>557.381</td>\n",
       "      <td>5162.766</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>21.245</td>\n",
       "      <td>563.110</td>\n",
       "      <td>5162.625</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>98.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>23.212</td>\n",
       "      <td>559.316</td>\n",
       "      <td>5165.277</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>92.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>22.261</td>\n",
       "      <td>558.059</td>\n",
       "      <td>5164.562</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>24.715</td>\n",
       "      <td>563.095</td>\n",
       "      <td>5164.957</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>95.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>22.113</td>\n",
       "      <td>557.381</td>\n",
       "      <td>5161.734</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>87.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>22.822</td>\n",
       "      <td>563.110</td>\n",
       "      <td>5165.398</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>88.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  fold  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  modeldata.csv    42     1                21.989        559.316         5150.520                       0.000012               90.5\n",
       "1  modeldata.csv    42     2                22.841        558.059         5151.535                       0.000014               91.4\n",
       "2  modeldata.csv    42     3                21.609        563.095         5153.730                       0.000011               96.5\n",
       "3  modeldata.csv    42     4                24.231        557.381         5162.766                       0.000014               95.0\n",
       "4  modeldata.csv    42     5                21.245        563.110         5162.625                       0.000011               98.5\n",
       "5  modeldata.csv   123     1                23.212        559.316         5165.277                       0.000012               92.2\n",
       "6  modeldata.csv   123     2                22.261        558.059         5164.562                       0.000012               88.5\n",
       "7  modeldata.csv   123     3                24.715        563.095         5164.957                       0.000011               95.3\n",
       "8  modeldata.csv   123     4                22.113        557.381         5161.734                       0.000014               87.6\n",
       "9  modeldata.csv   123     5                22.822        563.110         5165.398                       0.000010               88.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.842097</td>\n",
       "      <td>0.842635</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>10.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.813129</td>\n",
       "      <td>0.813005</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>10.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.796378</td>\n",
       "      <td>0.806069</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>10.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.816999</td>\n",
       "      <td>0.819401</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>10.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.810215</td>\n",
       "      <td>0.836625</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>10.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.842097</td>\n",
       "      <td>0.842635</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>10.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.813129</td>\n",
       "      <td>0.813005</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>10.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.796378</td>\n",
       "      <td>0.806069</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>10.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.816999</td>\n",
       "      <td>0.819401</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>10.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.843948</td>\n",
       "      <td>0.853223</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>10.833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  fold  accuracy        f1  precision    recall  runtime\n",
       "0  titanic.csv    42     1  0.843575  0.842097   0.842635  0.843575   10.817\n",
       "1  titanic.csv    42     2  0.814607  0.813129   0.813005  0.814607   10.766\n",
       "2  titanic.csv    42     3  0.803371  0.796378   0.806069  0.803371   10.889\n",
       "3  titanic.csv    42     4  0.820225  0.816999   0.819401  0.820225   10.852\n",
       "4  titanic.csv    42     5  0.820225  0.810215   0.836625  0.820225   10.769\n",
       "5  titanic.csv   123     1  0.843575  0.842097   0.842635  0.843575   10.923\n",
       "6  titanic.csv   123     2  0.814607  0.813129   0.813005  0.814607   10.825\n",
       "7  titanic.csv   123     3  0.803371  0.796378   0.806069  0.803371   10.836\n",
       "8  titanic.csv   123     4  0.820225  0.816999   0.819401  0.820225   10.777\n",
       "9  titanic.csv   123     5  0.848315  0.843948   0.853223  0.848315   10.833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_task3_runtime_memory_cv_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>10.817</td>\n",
       "      <td>19.342</td>\n",
       "      <td>4943.281</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>85.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>10.766</td>\n",
       "      <td>10.803</td>\n",
       "      <td>4943.699</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>81.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>10.889</td>\n",
       "      <td>26.200</td>\n",
       "      <td>4946.113</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>97.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>10.852</td>\n",
       "      <td>11.186</td>\n",
       "      <td>4946.180</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>81.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>10.769</td>\n",
       "      <td>27.632</td>\n",
       "      <td>4947.473</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>96.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>10.923</td>\n",
       "      <td>19.342</td>\n",
       "      <td>4948.660</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>87.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>10.825</td>\n",
       "      <td>10.802</td>\n",
       "      <td>4948.695</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>10.836</td>\n",
       "      <td>25.839</td>\n",
       "      <td>4948.199</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>99.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>10.777</td>\n",
       "      <td>11.198</td>\n",
       "      <td>4948.082</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>68.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>10.833</td>\n",
       "      <td>27.632</td>\n",
       "      <td>4948.629</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>67.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  fold  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  titanic.csv    42     1                10.817         19.342         4943.281                       0.000956               85.7\n",
       "1  titanic.csv    42     2                10.766         10.803         4943.699                       0.000181               81.5\n",
       "2  titanic.csv    42     3                10.889         26.200         4946.113                       0.000163               97.9\n",
       "3  titanic.csv    42     4                10.852         11.186         4946.180                       0.000163               81.3\n",
       "4  titanic.csv    42     5                10.769         27.632         4947.473                       0.000525               96.9\n",
       "5  titanic.csv   123     1                10.923         19.342         4948.660                       0.000967               87.4\n",
       "6  titanic.csv   123     2                10.825         10.802         4948.695                       0.000174               84.0\n",
       "7  titanic.csv   123     3                10.836         25.839         4948.199                       0.000242               99.1\n",
       "8  titanic.csv   123     4                10.777         11.198         4948.082                       0.000146               68.8\n",
       "9  titanic.csv   123     5                10.833         27.632         4948.629                       0.000524               67.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>6.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.897698</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>6.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>6.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>6.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.897698</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>6.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>6.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  fold  accuracy        f1  precision    recall  runtime\n",
       "0  train.csv    42     1  1.000000  1.000000   1.000000  1.000000    6.456\n",
       "1  train.csv    42     2  0.966667  0.966583   0.969697  0.966667    6.057\n",
       "2  train.csv    42     3  0.900000  0.897698   0.923077  0.900000    6.189\n",
       "3  train.csv    42     4  1.000000  1.000000   1.000000  1.000000    6.211\n",
       "4  train.csv    42     5  0.966667  0.966583   0.969697  0.966667    6.108\n",
       "5  train.csv   123     1  1.000000  1.000000   1.000000  1.000000    6.510\n",
       "6  train.csv   123     2  0.966667  0.966583   0.969697  0.966667    6.313\n",
       "7  train.csv   123     3  0.900000  0.897698   0.923077  0.900000    6.525\n",
       "8  train.csv   123     4  1.000000  1.000000   1.000000  1.000000    6.174\n",
       "9  train.csv   123     5  0.966667  0.966583   0.969697  0.966667    6.051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_task3_runtime_memory_cv_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>6.456</td>\n",
       "      <td>5.841</td>\n",
       "      <td>4720.391</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>6.057</td>\n",
       "      <td>3.679</td>\n",
       "      <td>4720.320</td>\n",
       "      <td>0.004243</td>\n",
       "      <td>69.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>6.189</td>\n",
       "      <td>3.001</td>\n",
       "      <td>4720.617</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>6.211</td>\n",
       "      <td>4.144</td>\n",
       "      <td>4721.281</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>72.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>6.108</td>\n",
       "      <td>3.292</td>\n",
       "      <td>4721.480</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>6.510</td>\n",
       "      <td>5.841</td>\n",
       "      <td>4721.254</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>70.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>6.313</td>\n",
       "      <td>3.679</td>\n",
       "      <td>4720.988</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>76.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>6.525</td>\n",
       "      <td>3.001</td>\n",
       "      <td>4721.289</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>6.174</td>\n",
       "      <td>4.144</td>\n",
       "      <td>4721.195</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>66.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>6.051</td>\n",
       "      <td>3.292</td>\n",
       "      <td>4721.414</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>69.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  fold  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  train.csv    42     1                 6.456          5.841         4720.391                       0.001400               72.0\n",
       "1  train.csv    42     2                 6.057          3.679         4720.320                       0.004243               69.7\n",
       "2  train.csv    42     3                 6.189          3.001         4720.617                       0.001207               66.1\n",
       "3  train.csv    42     4                 6.211          4.144         4721.281                       0.004459               72.6\n",
       "4  train.csv    42     5                 6.108          3.292         4721.480                       0.001145               66.4\n",
       "5  train.csv   123     1                 6.510          5.841         4721.254                       0.001367               70.8\n",
       "6  train.csv   123     2                 6.313          3.679         4720.988                       0.004611               76.6\n",
       "7  train.csv   123     3                 6.525          3.001         4721.289                       0.001100               69.0\n",
       "8  train.csv   123     4                 6.174          4.144         4721.195                       0.004401               66.9\n",
       "9  train.csv   123     5                 6.051          3.292         4721.414                       0.001169               69.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_cv_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972369</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>6.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.971471</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>6.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.972369</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>6.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.971471</td>\n",
       "      <td>0.973626</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>6.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  fold  accuracy        f1  precision    recall  runtime\n",
       "0  wine.csv    42     1  0.972222  0.972369   0.974747  0.972222    6.174\n",
       "1  wine.csv    42     2  1.000000  1.000000   1.000000  1.000000    6.246\n",
       "2  wine.csv    42     3  1.000000  1.000000   1.000000  1.000000    6.450\n",
       "3  wine.csv    42     4  0.971429  0.971471   0.973626  0.971429    6.786\n",
       "4  wine.csv    42     5  1.000000  1.000000   1.000000  1.000000    6.777\n",
       "5  wine.csv   123     1  0.972222  0.972369   0.974747  0.972222    6.681\n",
       "6  wine.csv   123     2  1.000000  1.000000   1.000000  1.000000    6.444\n",
       "7  wine.csv   123     3  1.000000  1.000000   1.000000  1.000000    6.647\n",
       "8  wine.csv   123     4  0.971429  0.971471   0.973626  0.971429    6.981\n",
       "9  wine.csv   123     5  1.000000  1.000000   1.000000  1.000000    7.059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_task3_runtime_memory_cv_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>training_runtime_sec</th>\n",
       "      <th>model_size_MB</th>\n",
       "      <th>memory_usage_MB</th>\n",
       "      <th>inference_time_per_sample_sec</th>\n",
       "      <th>cpu_usage_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>6.174</td>\n",
       "      <td>4.408</td>\n",
       "      <td>4721.789</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>67.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>6.246</td>\n",
       "      <td>5.012</td>\n",
       "      <td>4721.887</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>6.450</td>\n",
       "      <td>4.468</td>\n",
       "      <td>4721.273</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>68.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>6.786</td>\n",
       "      <td>6.647</td>\n",
       "      <td>4722.598</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>76.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>6.777</td>\n",
       "      <td>4.741</td>\n",
       "      <td>4722.082</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>92.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>6.681</td>\n",
       "      <td>4.408</td>\n",
       "      <td>4722.672</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>94.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>6.444</td>\n",
       "      <td>5.012</td>\n",
       "      <td>4722.648</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>6.647</td>\n",
       "      <td>4.468</td>\n",
       "      <td>4722.785</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>92.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>6.981</td>\n",
       "      <td>6.647</td>\n",
       "      <td>4722.625</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>7.059</td>\n",
       "      <td>4.741</td>\n",
       "      <td>4722.852</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>94.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  fold  training_runtime_sec  model_size_MB  memory_usage_MB  inference_time_per_sample_sec  cpu_usage_percent\n",
       "0  wine.csv    42     1                 6.174          4.408         4721.789                       0.003797               67.6\n",
       "1  wine.csv    42     2                 6.246          5.012         4721.887                       0.000695               70.0\n",
       "2  wine.csv    42     3                 6.450          4.468         4721.273                       0.001111               68.9\n",
       "3  wine.csv    42     4                 6.786          6.647         4722.598                       0.003387               76.2\n",
       "4  wine.csv    42     5                 6.777          4.741         4722.082                       0.001115               92.6\n",
       "5  wine.csv   123     1                 6.681          4.408         4722.672                       0.003723               94.1\n",
       "6  wine.csv   123     2                 6.444          5.012         4722.648                       0.000639               94.6\n",
       "7  wine.csv   123     3                 6.647          4.468         4722.785                       0.001056               92.6\n",
       "8  wine.csv   123     4                 6.981          6.647         4722.625                       0.004516               94.6\n",
       "9  wine.csv   123     5                 7.059          4.741         4722.852                       0.001086               94.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in data_dict_classification_only.keys():\n",
    "    dataset_base = dataset_name.replace('.csv', '')\n",
    "    csv_file1 = os.path.join(CV_FOLDS_METRICS_PATH, f\"autogluon_{dataset_base}_cv_summary.csv\")\n",
    "    csv_file2 = os.path.join(RUNTIME_MEMORY_TASK3_PATH, f\"autogluon_{dataset_base}_task3_runtime_memory_cv_metrics.csv\")\n",
    "    json_file1 = os.path.join(RUNTIME_MEMORY_TASK3_PATH, f\"autogluon_{dataset_base}_runtime_memory_metrics_cv_in_json.json\")\n",
    "\n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_cv_summary.csv\")\n",
    "    df1 = pd.read_csv(csv_file1)\n",
    "    display(df1)\n",
    "\n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_task3_runtime_memory_cv_metrics.csv\")\n",
    "    df2 = pd.read_csv(csv_file2)\n",
    "    display(df2)\n",
    "    \n",
    "    # print(f\"JSON for {dataset_base}: autogluon_{dataset_base}_runtime_memory_cv_metrics_in_json.json\")\n",
    "    # df3 = pd.read_json(json_file1)\n",
    "    # display(df3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b47fe26-5a07-4067-b503-34fee95148a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbz87539v\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       9.13 GB / 31.92 GB (28.6%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbz87539v\"\n",
      "Train Data Rows:    81045\n",
      "Train Data Columns: 113\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10005.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.87 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 110 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 14): ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_7', 'f_8', 'f_10', 'f_13', 'f_14', 'f_15', 'f_21', 'f_32', 'f_62']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 14 | ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 99 | ['f_0', 'f_6', 'f_9', 'f_11', 'f_12', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  3 | ['f_12', 'f_81', 'f_102']\n",
      "\t\t('int', ['bool']) : 96 | ['f_0', 'f_6', 'f_9', 'f_11', 'f_16', ...]\n",
      "\t2.1s = Fit runtime\n",
      "\t99 features in original data used to generate 99 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03084706027515578, Train Rows: 78545, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 7.88s of the 7.88s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/7.6 GB\n",
      "\t0.9228\t = Validation score   (accuracy)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.25s of the 1.24s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/6.3 GB\n",
      "\t0.9244\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 0.71s of the 0.71s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/6.2 GB\n",
      "\tWarning: Model is expected to require 11.9s to train, which exceeds the maximum time limit of 0.6s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini.\n",
      "Fitting model: RandomForestEntr ... Training model for up to 0.26s of the 0.26s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/6.2 GB\n",
      "\tWarning: Model is expected to require 11.6s to train, which exceeds the maximum time limit of 0.2s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 7.88s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t0.9244\t = Validation score   (accuracy)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 404777.5 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbz87539v\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmprrrcv4om\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       6.04 GB / 31.92 GB (18.9%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmprrrcv4om\"\n",
      "Train Data Rows:    81045\n",
      "Train Data Columns: 114\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6197.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.49 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 111 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 23): ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_7', 'f_8', 'f_10', 'f_11', 'f_12', 'f_13', 'f_15', 'f_17', 'f_19', 'f_26', 'f_31', 'f_32', 'f_35', 'f_37', 'f_43', 'f_47', 'f_51', 'f_87']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 23 | ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 91 | ['f_0', 'f_6', 'f_9', 'f_14', 'f_16', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  3 | ['f_88', 'f_102', 'f_108']\n",
      "\t\t('int', ['bool']) : 88 | ['f_0', 'f_6', 'f_9', 'f_14', 'f_16', ...]\n",
      "\t2.4s = Fit runtime\n",
      "\t91 features in original data used to generate 91 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03084706027515578, Train Rows: 78545, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 7.48s of the 7.48s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/5.9 GB\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 7.11s of the 7.11s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/6.0 GB\n",
      "\t0.928\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 6.72s of the 6.72s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/6.0 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 192 due to low time. Expected time usage reduced from 10.3s -> 6.6s...\n",
      "\t0.928\t = Validation score   (accuracy)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3.90s of the 3.89s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/5.7 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 109 due to low time. Expected time usage reduced from 10.5s -> 3.8s...\n",
      "\t0.9256\t = Validation score   (accuracy)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 2.26s of the 2.26s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\tRan out of time, early stopping on iteration 171.\n",
      "\t0.9252\t = Validation score   (accuracy)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 7.49s of the -0.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.14s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 616882.0 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmprrrcv4om\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpf8qpdd_7\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       5.18 GB / 31.92 GB (16.2%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpf8qpdd_7\"\n",
      "Train Data Rows:    81045\n",
      "Train Data Columns: 113\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5225.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.87 MB (1.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 110 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 14): ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_7', 'f_8', 'f_10', 'f_13', 'f_14', 'f_15', 'f_21', 'f_32', 'f_62']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 14 | ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 99 | ['f_0', 'f_6', 'f_9', 'f_11', 'f_12', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  3 | ['f_12', 'f_81', 'f_102']\n",
      "\t\t('int', ['bool']) : 96 | ['f_0', 'f_6', 'f_9', 'f_11', 'f_16', ...]\n",
      "\t2.3s = Fit runtime\n",
      "\t99 features in original data used to generate 99 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.27 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03084706027515578, Train Rows: 78545, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 7.68s of the 7.67s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/5.4 GB\n",
      "\t0.9228\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 7.13s of the 7.12s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4.9 GB\n",
      "\t0.9244\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 6.51s of the 6.51s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/5.0 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 129 due to low time. Expected time usage reduced from 14.8s -> 6.4s...\n",
      "\t0.914\t = Validation score   (accuracy)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/5.0 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 95 due to low time. Expected time usage reduced from 13.1s -> 4.2s...\n",
      "\t0.9156\t = Validation score   (accuracy)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 2.45s of the 2.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.922\t = Validation score   (accuracy)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1.14s of the 1.14s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/5.4 GB\n",
      "\tWarning: Model is expected to require 9.1s to train, which exceeds the maximum time limit of 1.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini.\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 0.82s of the 0.82s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/5.4 GB\n",
      "\tWarning: Model is expected to require 11.9s to train, which exceeds the maximum time limit of 0.8s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesEntr.\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 0.44s of the 0.44s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/5.1 GB\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 7.68s of the -1.03s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.667, 'RandomForestEntr': 0.333}\n",
      "\t0.9264\t = Validation score   (accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 11.15s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 71023.9 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpf8qpdd_7\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1hypd885\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.32 GB / 31.92 GB (13.5%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1hypd885\"\n",
      "Train Data Rows:    81045\n",
      "Train Data Columns: 114\n",
      "Label Column:       IsInsurable\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4320.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.49 MB (1.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 111 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 23): ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_7', 'f_8', 'f_10', 'f_11', 'f_12', 'f_13', 'f_15', 'f_17', 'f_19', 'f_26', 'f_31', 'f_32', 'f_35', 'f_37', 'f_43', 'f_47', 'f_51', 'f_87']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 23 | ['f_1', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 91 | ['f_0', 'f_6', 'f_9', 'f_14', 'f_16', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  3 | ['f_88', 'f_102', 'f_108']\n",
      "\t\t('int', ['bool']) : 88 | ['f_0', 'f_6', 'f_9', 'f_14', 'f_16', ...]\n",
      "\t2.5s = Fit runtime\n",
      "\t91 features in original data used to generate 91 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.66 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.03084706027515578, Train Rows: 78545, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 7.47s of the 7.46s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4.1 GB\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 6.85s of the 6.84s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/4.3 GB\n",
      "\t0.928\t = Validation score   (accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 6.33s of the 6.32s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/4.4 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 134 due to low time. Expected time usage reduced from 14.0s -> 6.3s...\n",
      "\t0.9276\t = Validation score   (accuracy)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 4.20s of the 4.20s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/4.1 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 107 due to low time. Expected time usage reduced from 11.6s -> 4.2s...\n",
      "\t0.9264\t = Validation score   (accuracy)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 2.61s of the 2.60s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9252\t = Validation score   (accuracy)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 0.17s of the 0.17s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0, mem=0.0/3.9 GB\n",
      "\tWarning: Model is expected to require 29.2s to train, which exceeds the maximum time limit of 0.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 7.47s of the -0.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t0.9296\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 10.71s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 274209.2 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp1hypd885\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for dataset: modeldata.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - LightGBM</td>\n",
       "      <td>0.919218</td>\n",
       "      <td>0.918952</td>\n",
       "      <td>0.918885</td>\n",
       "      <td>0.919218</td>\n",
       "      <td>11.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.922437</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>10.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.918336</td>\n",
       "      <td>0.918251</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>12.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMXT</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.922437</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>11.565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  fold                       model_name  accuracy        f1  precision    recall  runtime\n",
       "0  modeldata.csv    42     1             autogluon - LightGBM  0.919218  0.918952   0.918885  0.919218   11.265\n",
       "1  modeldata.csv    42     2  autogluon - WeightedEnsemble_L2  0.922759  0.922470   0.922437  0.922759   10.848\n",
       "2  modeldata.csv   123     1  autogluon - WeightedEnsemble_L2  0.918551  0.918336   0.918251  0.918551   12.173\n",
       "3  modeldata.csv   123     2           autogluon - LightGBMXT  0.922759  0.922470   0.922437  0.922759   11.565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptlsd5n64\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.48 GB / 31.92 GB (14.0%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmptlsd5n64\"\n",
      "Train Data Rows:    445\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4524.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['f_8']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['f_8']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 10 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 7 | ['f_0', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\t\t('int', ['bool']) : 3 | ['f_1', 'f_9', 'f_10']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 356, Val Rows: 89\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.90s of the 9.90s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.65s of the 9.65s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.6 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.44s of the 9.43s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.84s of the 8.83s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 8.25s of the 8.25s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.92s of the 7.92s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.33s of the 7.33s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 6.73s of the 6.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.6 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.21s of the 6.20s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.83s of the 5.83s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.5 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t2.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3.34s of the 3.34s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.5 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.90s of the 2.75s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7.35s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12711.7 rows/s (89 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (89 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmptlsd5n64\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxw0e6q8m\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.50 GB / 31.92 GB (14.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxw0e6q8m\"\n",
      "Train Data Rows:    446\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4463.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['f_9', 'f_10']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['f_9', 'f_10']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 9 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 7 | ['f_0', 'f_3', 'f_4', 'f_5', 'f_6', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['f_1', 'f_2']\n",
      "\t0.2s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 356, Val Rows: 90\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.81s of the 9.80s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.41s of the 9.41s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.3 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.12s of the 9.12s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.26s of the 8.26s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.58s of the 7.58s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.24s of the 7.24s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.62s of the 6.62s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 5.99s of the 5.99s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.57s of the 5.57s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.40s of the 5.40s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8111\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.60s of the 4.59s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.81s of the 4.29s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.83s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12840.1 rows/s (90 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (90 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpxw0e6q8m\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpl3210dj8\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.39 GB / 31.92 GB (13.7%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpl3210dj8\"\n",
      "Train Data Rows:    445\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4478.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['f_8']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['f_8']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 10 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 7 | ['f_0', 'f_2', 'f_3', 'f_4', 'f_5', ...]\n",
      "\t\t('int', ['bool']) : 3 | ['f_1', 'f_9', 'f_10']\n",
      "\t0.1s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 356, Val Rows: 89\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.92s of the 9.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.68s of the 9.68s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.43s of the 9.43s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.77s of the 8.77s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 8.14s of the 8.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.70s of the 7.69s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.99s of the 6.99s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8202\t = Validation score   (accuracy)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 6.24s of the 6.24s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.2 GB\n",
      "No improvement since epoch 4: early stopping\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.78s of the 5.78s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 5.53s of the 5.53s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.3 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.77s of the 4.77s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.3 GB\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.92s of the 4.44s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.8427\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.67s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6255.6 rows/s (89 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (89 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpl3210dj8\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp10mc53ay\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.41 GB / 31.92 GB (13.8%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp10mc53ay\"\n",
      "Train Data Rows:    446\n",
      "Train Data Columns: 11\n",
      "Label Column:       Survived\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4478.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.04 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['f_9', 'f_10']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['f_9', 'f_10']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 9 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 7 | ['f_0', 'f_3', 'f_4', 'f_5', 'f_6', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['f_1', 'f_2']\n",
      "\t0.1s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 356, Val Rows: 90\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.87s of the 9.87s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.63s of the 9.63s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.38s of the 9.38s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.78s of the 8.78s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 8.20s of the 8.20s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.88s of the 7.88s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.31s of the 7.31s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 6.73s of the 6.73s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.32s of the 6.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.16s of the 6.16s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8111\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.37s of the 5.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.87s of the 5.00s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.11s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 14996.3 rows/s (90 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (90 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp10mc53ay\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for dataset: titanic.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>0.810628</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>8.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - NeuralNetFastAI</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>0.782462</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>6.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>0.810628</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>6.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - NeuralNetFastAI</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>0.782462</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>5.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  fold                   model_name  accuracy        f1  precision    recall  runtime\n",
       "0  titanic.csv    42     1         autogluon - CatBoost  0.818386  0.810628   0.826018  0.818386    8.046\n",
       "1  titanic.csv    42     2  autogluon - NeuralNetFastAI  0.784270  0.782954   0.782462  0.784270    6.538\n",
       "2  titanic.csv   123     1         autogluon - CatBoost  0.818386  0.810628   0.826018  0.818386    6.241\n",
       "3  titanic.csv   123     2  autogluon - NeuralNetFastAI  0.784270  0.782954   0.782462  0.784270    5.653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbytx0qmp\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.41 GB / 31.92 GB (13.8%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbytx0qmp\"\n",
      "Train Data Rows:    75\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4526.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 60, Val Rows: 15\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.60s of the 9.60s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.37s of the 9.37s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.14s of the 9.13s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.51s of the 8.50s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.88s of the 7.88s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.70s of the 7.70s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.08s of the 7.07s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.48s of the 6.48s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.38s of the 6.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.14s of the 6.14s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 5.90s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2737.4 rows/s (15 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpbytx0qmp\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcofsvzao\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.43 GB / 31.92 GB (13.9%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcofsvzao\"\n",
      "Train Data Rows:    75\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4493.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 60, Val Rows: 15\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.66s of the 9.65s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.44s of the 9.44s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.4 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.19s of the 9.19s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.57s of the 8.57s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.97s of the 7.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.76s of the 7.76s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.17s of the 7.17s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.58s of the 6.58s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.29s of the 6.29s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.04s of the 6.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 5.79s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.32s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7486.3 rows/s (15 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpcofsvzao\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7d_xby5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.81 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7d_xby5\"\n",
      "Train Data Rows:    75\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4928.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 60, Val Rows: 15\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.61s of the 9.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.38s of the 9.38s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.16s of the 9.16s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.53s of the 8.53s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.92s of the 7.92s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.74s of the 7.74s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.14s of the 7.14s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.53s of the 6.53s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.43s of the 6.43s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.19s of the 6.19s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 5.95s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 1.0}\n",
      "\t0.9333\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.15s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2999.4 rows/s (15 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpb7d_xby5\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp1rfx_lf\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.82 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp1rfx_lf\"\n",
      "Train Data Rows:    75\n",
      "Train Data Columns: 4\n",
      "Label Column:       variety\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4929.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4 | ['f_0', 'f_1', 'f_2', 'f_3']\n",
      "\t0.0s = Fit runtime\n",
      "\t4 features in original data used to generate 4 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.00 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 60, Val Rows: 15\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.97s of the 9.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.66s of the 9.66s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.45s of the 9.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.24s of the 9.24s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.63s of the 8.63s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 8.03s of the 8.03s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.82s of the 7.82s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.23s of the 7.23s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.61s of the 6.61s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.50s of the 6.50s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.22s of the 6.22s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.8\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.97s of the 5.99s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t0.8667\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.12s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7499.6 rows/s (15 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpp1rfx_lf\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for dataset: train.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>4.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.918846</td>\n",
       "      <td>0.919656</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>4.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>4.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.918846</td>\n",
       "      <td>0.919656</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>4.742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  fold            model_name  accuracy        f1  precision    recall  runtime\n",
       "0  train.csv    42     1  autogluon - CatBoost  0.946667  0.946581   0.948470  0.946667    4.816\n",
       "1  train.csv    42     2  autogluon - CatBoost  0.920000  0.918846   0.919656  0.920000    4.962\n",
       "2  train.csv   123     1  autogluon - CatBoost  0.946667  0.946581   0.948470  0.946667    4.805\n",
       "3  train.csv   123     2  autogluon - CatBoost  0.920000  0.918846   0.919656  0.920000    4.742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpvu4l89cn\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.83 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpvu4l89cn\"\n",
      "Train Data Rows:    89\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4947.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.05s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 71, Val Rows: 18\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.95s of the 9.95s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.53s of the 9.53s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.30s of the 9.29s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.06s of the 9.06s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.43s of the 8.43s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.80s of the 7.80s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.59s of the 7.58s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.96s of the 6.96s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.36s of the 6.36s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.25s of the 6.25s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.81s of the 5.81s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.95s of the 5.47s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 366.9 rows/s (18 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpvu4l89cn\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp4nj18387\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.82 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmp4nj18387\"\n",
      "Train Data Rows:    89\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4936.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 71, Val Rows: 18\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.45s of the 9.45s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.23s of the 9.22s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 8.99s of the 8.99s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.38s of the 8.38s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.79s of the 7.79s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.56s of the 7.56s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.95s of the 6.95s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.36s of the 6.36s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.27s of the 6.26s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.98s of the 5.97s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 5.68s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.42s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2571.1 rows/s (18 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmp4nj18387\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpwj2yp0jc\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.82 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpwj2yp0jc\"\n",
      "Train Data Rows:    89\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4927.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 71, Val Rows: 18\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.52s of the 9.52s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.29s of the 9.29s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.04s of the 9.04s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.41s of the 8.40s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.78s of the 7.78s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.58s of the 7.58s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 7.00s of the 7.00s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.41s of the 6.41s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.32s of the 6.32s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.91s of the 5.91s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 5.61s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesGini': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.5s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 375.7 rows/s (18 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpwj2yp0jc\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgqz0q_iq\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26200\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.81 GB / 31.92 GB (15.1%)\n",
      "Disk Space Avail:   92.34 GB / 476.13 GB (19.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgqz0q_iq\"\n",
      "Train Data Rows:    89\n",
      "Train Data Columns: 13\n",
      "Label Column:       Wine\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4936.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 71, Val Rows: 18\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 9.96s of the 9.96s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 9.49s of the 9.49s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 9.26s of the 9.26s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 9.03s of the 9.03s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 8.40s of the 8.40s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 7.75s of the 7.75s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.9444\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 7.51s of the 7.51s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8333\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 6.91s of the 6.91s of remaining time.\n",
      "\tFitting with cpus=16, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 6.28s of the 6.28s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t0.8889\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 6.17s of the 6.17s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 5.88s of the 5.88s of remaining time.\n",
      "\tFitting with cpus=8, gpus=0, mem=0.0/4.8 GB\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.96s of the 5.58s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.53s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2570.9 rows/s (18 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\user\\AppData\\Local\\Temp\\tmpgqz0q_iq\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for dataset: wine.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>0.954885</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>5.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMLarge</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.898967</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>5.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>0.954885</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>5.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMLarge</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.898967</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>5.186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  fold                 model_name  accuracy        f1  precision    recall  runtime\n",
       "0  wine.csv    42     1       autogluon - CatBoost  0.955056  0.954885   0.955118  0.955056    5.387\n",
       "1  wine.csv    42     2  autogluon - LightGBMLarge  0.898876  0.898967   0.899390  0.898876    5.186\n",
       "2  wine.csv   123     1       autogluon - CatBoost  0.955056  0.954885   0.955118  0.955056    5.177\n",
       "3  wine.csv   123     2  autogluon - LightGBMLarge  0.898876  0.898967   0.899390  0.898876    5.186"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Task 6 ----------------\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "def evaluate_pipeline_folds_times_autogluon(leakage_free_final_output):\n",
    "    for dataset_name, folds in leakage_free_final_output.items():\n",
    "        dataset_base = dataset_name.replace('.csv', '')\n",
    "\n",
    "        dataset_summary = []\n",
    "        time_log_rows = []\n",
    "\n",
    "        target_col = TARGET_COLS[dataset_name]\n",
    "\n",
    "        for seed in RANDOM_SEEDS:\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "            for fold_name, split in folds.items():\n",
    "\n",
    "                df_train = split['train'].copy()\n",
    "                df_val = split['val'].copy()\n",
    "                pipe_times = split['pipeline_time']\n",
    "                pipe_times_copy = pipe_times.copy()\n",
    "                pipe_times_copy[\"total_time\"] = sum(pipe_times_copy.values())\n",
    "\n",
    "                X_train = df_train.drop(columns=[target_col])\n",
    "                y_train = df_train[target_col]\n",
    "\n",
    "                X_val = df_val.drop(columns=[target_col])\n",
    "                y_val = df_val[target_col]\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "                temp_dir = tempfile.mkdtemp()\n",
    "                start = time.time()\n",
    "\n",
    "                df_train_proc = X_train.copy()\n",
    "                df_train_proc[target_col] = y_train.values\n",
    "\n",
    "                predictor = TabularPredictor(\n",
    "                    label=target_col,\n",
    "                    problem_type=tasks_dict_classification_only[dataset_name],\n",
    "                    path=temp_dir\n",
    "                ).fit(df_train_proc, time_limit=CV_TIME_BUDGET)\n",
    "\n",
    "                preds = predictor.predict(X_val)\n",
    "                runtime = round(time.time() - start, 3)\n",
    "                fold_idx = int(fold_name.split(\"_\")[1])\n",
    "\n",
    "                best_model_raw = predictor.leaderboard(silent=True).iloc[0][\"model\"]\n",
    "                best_model = f\"autogluon - {best_model_raw}\"\n",
    "\n",
    "                try:\n",
    "                    model_obj = predictor._trainer.load_model(best_model_raw)\n",
    "                    generate_shap_explanations(\n",
    "                        model=model_obj,\n",
    "                        X=X_val,\n",
    "                        model_name=best_model_raw.replace(\"/\", \"_\"),\n",
    "                        save_base_path=AUTOGUON_SHAP_PATH\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                dataset_summary.append({\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"seed\": seed,\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"model_name\": best_model,\n",
    "                    \"accuracy\": (accuracy_score(y_val, preds)),\n",
    "                    \"f1\": (f1_score(y_val, preds, average='weighted')),\n",
    "                    \"precision\": (precision_score(y_val, preds, average='weighted')),\n",
    "                    \"recall\": (recall_score(y_val, preds, average='weighted')),\n",
    "                    \"runtime\": runtime\n",
    "                })\n",
    "\n",
    "                row = {\"dataset\": dataset_name, \"fold\": fold_idx, \"model_name\": best_model}\n",
    "                row.update(pipe_times_copy)\n",
    "                time_log_rows.append(row)\n",
    "\n",
    "                shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "        df_summary = pd.DataFrame(dataset_summary).drop_duplicates()\n",
    "        df_times = pd.DataFrame(time_log_rows).drop_duplicates()\n",
    "        \n",
    "        print(f\"\\nSummary for dataset: {dataset_name}\")\n",
    "        display(df_summary)\n",
    "\n",
    "        df_summary = df_summary.sort_values([\"dataset\", \"seed\", \"fold\"])\n",
    "        df_times = df_times.sort_values([\"dataset\", \"fold\"])\n",
    "\n",
    "        summary_path = os.path.join(\n",
    "            PIPELINE_DATAFRAMES_METRICS_PATH,\n",
    "            f\"autogluon_{dataset_base}_pipeline_summary.csv\")\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "\n",
    "        time_log_path = os.path.join(\n",
    "            PIPELINE_LOG_TIMES_PATH,\n",
    "            f\"autogluon_{dataset_base}_pipeline_time_log.csv\")\n",
    "        df_times.to_csv(time_log_path, index=False)\n",
    "\n",
    "\n",
    "evaluate_pipeline_folds_times_autogluon(run_pipeline_on_folds_with_control(pipeline_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94db4330-471d-4da5-9c3e-96c33db57bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_pipeline_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - LightGBM</td>\n",
       "      <td>0.919218</td>\n",
       "      <td>0.918952</td>\n",
       "      <td>0.918885</td>\n",
       "      <td>0.919218</td>\n",
       "      <td>11.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.922437</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>10.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.918336</td>\n",
       "      <td>0.918251</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>12.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMXT</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.922437</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>11.565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  seed  fold                       model_name  accuracy        f1  precision    recall  runtime\n",
       "0  modeldata.csv    42     1             autogluon - LightGBM  0.919218  0.918952   0.918885  0.919218   11.265\n",
       "1  modeldata.csv    42     2  autogluon - WeightedEnsemble_L2  0.922759  0.922470   0.922437  0.922759   10.848\n",
       "2  modeldata.csv   123     1  autogluon - WeightedEnsemble_L2  0.918551  0.918336   0.918251  0.918551   12.173\n",
       "3  modeldata.csv   123     2           autogluon - LightGBMXT  0.922759  0.922470   0.922437  0.922759   11.565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for modeldata: autogluon_modeldata_pipeline_time_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>nan_guard_before_vif</th>\n",
       "      <th>vif</th>\n",
       "      <th>binner</th>\n",
       "      <th>nan_guard_before_poly</th>\n",
       "      <th>selector</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - LightGBM</td>\n",
       "      <td>0.525178</td>\n",
       "      <td>0.182331</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>0.024257</td>\n",
       "      <td>0.889658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.525178</td>\n",
       "      <td>0.182331</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>0.024257</td>\n",
       "      <td>0.889658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - WeightedEnsemble_L2</td>\n",
       "      <td>0.518440</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.080931</td>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.966809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modeldata.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMXT</td>\n",
       "      <td>0.518440</td>\n",
       "      <td>0.202923</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.080931</td>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.966809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset  fold                       model_name  preprocessing  nan_guard_before_vif       vif    binner  nan_guard_before_poly  selector  total_time\n",
       "0  modeldata.csv     1             autogluon - LightGBM       0.525178              0.182331  0.013367  0.084668               0.059857  0.024257    0.889658\n",
       "1  modeldata.csv     1  autogluon - WeightedEnsemble_L2       0.525178              0.182331  0.013367  0.084668               0.059857  0.024257    0.889658\n",
       "2  modeldata.csv     2  autogluon - WeightedEnsemble_L2       0.518440              0.202923  0.020153  0.097889               0.080931  0.046473    0.966809\n",
       "3  modeldata.csv     2           autogluon - LightGBMXT       0.518440              0.202923  0.020153  0.097889               0.080931  0.046473    0.966809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_pipeline_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>0.810628</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>8.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - NeuralNetFastAI</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>0.782462</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>6.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>0.810628</td>\n",
       "      <td>0.826018</td>\n",
       "      <td>0.818386</td>\n",
       "      <td>6.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - NeuralNetFastAI</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>0.782954</td>\n",
       "      <td>0.782462</td>\n",
       "      <td>0.784270</td>\n",
       "      <td>5.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  seed  fold                   model_name  accuracy        f1  precision    recall  runtime\n",
       "0  titanic.csv    42     1         autogluon - CatBoost  0.818386  0.810628   0.826018  0.818386    8.046\n",
       "1  titanic.csv    42     2  autogluon - NeuralNetFastAI  0.784270  0.782954   0.782462  0.784270    6.538\n",
       "2  titanic.csv   123     1         autogluon - CatBoost  0.818386  0.810628   0.826018  0.818386    6.241\n",
       "3  titanic.csv   123     2  autogluon - NeuralNetFastAI  0.784270  0.782954   0.782462  0.784270    5.653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for titanic: autogluon_titanic_pipeline_time_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>nan_guard_before_vif</th>\n",
       "      <th>vif</th>\n",
       "      <th>binner</th>\n",
       "      <th>nan_guard_before_poly</th>\n",
       "      <th>poly</th>\n",
       "      <th>selector</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.015213</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.035431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - NeuralNetFastAI</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.041645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset  fold                   model_name  preprocessing  nan_guard_before_vif       vif    binner  nan_guard_before_poly      poly  selector  total_time\n",
       "0  titanic.csv     1         autogluon - CatBoost       0.010138              0.003020  0.001006  0.002011               0.001006  0.015213  0.003038    0.035431\n",
       "1  titanic.csv     2  autogluon - NeuralNetFastAI       0.010440              0.004032  0.001002  0.002013               0.000000  0.017115  0.007042    0.041645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_pipeline_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>4.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.918846</td>\n",
       "      <td>0.919656</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>4.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>4.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.918846</td>\n",
       "      <td>0.919656</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>4.742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  seed  fold            model_name  accuracy        f1  precision    recall  runtime\n",
       "0  train.csv    42     1  autogluon - CatBoost  0.946667  0.946581   0.948470  0.946667    4.816\n",
       "1  train.csv    42     2  autogluon - CatBoost  0.920000  0.918846   0.919656  0.920000    4.962\n",
       "2  train.csv   123     1  autogluon - CatBoost  0.946667  0.946581   0.948470  0.946667    4.805\n",
       "3  train.csv   123     2  autogluon - CatBoost  0.920000  0.918846   0.919656  0.920000    4.742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for train: autogluon_train_pipeline_time_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>nan_guard_before_vif</th>\n",
       "      <th>vif</th>\n",
       "      <th>binner</th>\n",
       "      <th>nan_guard_before_poly</th>\n",
       "      <th>poly</th>\n",
       "      <th>selector</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  fold            model_name  preprocessing  nan_guard_before_vif       vif    binner  nan_guard_before_poly  poly  selector  total_time\n",
       "0  train.csv     1  autogluon - CatBoost       0.002004                   0.0  0.001002  0.000000                    0.0   0.0       0.0    0.003006\n",
       "1  train.csv     2  autogluon - CatBoost       0.002004                   0.0  0.001002  0.001003                    0.0   0.0       0.0    0.004009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_pipeline_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>seed</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>0.954885</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>5.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMLarge</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.898967</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>5.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>0.954885</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.955056</td>\n",
       "      <td>5.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>123</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMLarge</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.898967</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>5.186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  seed  fold                 model_name  accuracy        f1  precision    recall  runtime\n",
       "0  wine.csv    42     1       autogluon - CatBoost  0.955056  0.954885   0.955118  0.955056    5.387\n",
       "1  wine.csv    42     2  autogluon - LightGBMLarge  0.898876  0.898967   0.899390  0.898876    5.186\n",
       "2  wine.csv   123     1       autogluon - CatBoost  0.955056  0.954885   0.955118  0.955056    5.177\n",
       "3  wine.csv   123     2  autogluon - LightGBMLarge  0.898876  0.898967   0.899390  0.898876    5.186"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV for wine: autogluon_wine_pipeline_time_log.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>fold</th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>nan_guard_before_vif</th>\n",
       "      <th>vif</th>\n",
       "      <th>binner</th>\n",
       "      <th>nan_guard_before_poly</th>\n",
       "      <th>poly</th>\n",
       "      <th>selector</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>autogluon - CatBoost</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>autogluon - LightGBMLarge</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  fold                 model_name  preprocessing  nan_guard_before_vif       vif    binner  nan_guard_before_poly      poly  selector  total_time\n",
       "0  wine.csv     1       autogluon - CatBoost       0.003201                   0.0  0.000000  0.001031                    0.0  0.001009       0.0    0.005240\n",
       "1  wine.csv     2  autogluon - LightGBMLarge       0.003007                   0.0  0.001002  0.001002                    0.0  0.001002       0.0    0.006013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in data_dict_classification_only.keys():\n",
    "    dataset_base = dataset_name.replace('.csv', '')\n",
    "    \n",
    "    csv_file_1 = os.path.join(PIPELINE_DATAFRAMES_METRICS_PATH, f\"autogluon_{dataset_base}_pipeline_summary.csv\")\n",
    "    csv_file_2 = os.path.join(PIPELINE_LOG_TIMES_PATH, f\"autogluon_{dataset_base}_pipeline_time_log.csv\")\n",
    "    \n",
    "    df1 = pd.read_csv(csv_file_1)\n",
    "    df2 = pd.read_csv(csv_file_2)\n",
    "    \n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_pipeline_summary.csv\")\n",
    "    display(df1) \n",
    "    print(f\"CSV for {dataset_base}: autogluon_{dataset_base}_pipeline_time_log.csv\")\n",
    "    display(df2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aeb721-b9e2-4f68-b8c9-45de2e70b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML - AutoGluon",
   "language": "python",
   "name": "automl_autogluon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
